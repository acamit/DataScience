{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1ktRncs6gXB"
   },
   "source": [
    "# EYE FOR BLIND\n",
    "This notebook will be used to prepare the capstone project 'Eye for Blind'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4neCnw-i6gXF",
    "outputId": "66e9d609-49d5-4d04-9051-e6b3ff3f8fd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n",
      "/content/gdrive/My Drive/Colab Notebooks\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# import os\n",
    "# os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Colab Notebooks\"\n",
    "\n",
    "# %cd /content/gdrive/My Drive/Colab Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LYsbt_Ry6gXF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import all the required libraries\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import pathlib, os\n",
    "from collections import Counter , OrderedDict\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import collections\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing.image import load_img\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import time\n",
    "\n",
    "# image processing\n",
    "from skimage import io\n",
    "from skimage.transform import rescale, resize\n",
    "\n",
    "# import cv2\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVukGSvr6gXG"
   },
   "source": [
    "Let's read the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3U4vxRyV6gXG"
   },
   "source": [
    "## Data understanding\n",
    "1.Import the dataset and read image & captions into two seperate variables\n",
    "\n",
    "2.Visualise both the images & text present in the dataset\n",
    "\n",
    "3.Create word-to-index and index-to-word mappings.\n",
    "\n",
    "4.Create a dataframe which summarizes the image, path & captions as a dataframe\n",
    "\n",
    "5.Visualise the top 30 occuring words in the captions\n",
    "\n",
    "6.Create a list which contains all the captions & path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jEp82rNO6gXH",
    "outputId": "0ef79aa1-324d-43f8-b80c-7af89481458a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Flickr8K/Images/2201222219_8d656b0633.jpg', 'Flickr8K/Images/3681172959_6674c118d2.jpg', 'Flickr8K/Images/3257107194_f235c8f7ab.jpg']\n",
      "The total images present in the dataset: 8091\n"
     ]
    }
   ],
   "source": [
    "#Import the dataset and read the image into a seperate variable\n",
    "\n",
    "images='Flickr8K/Images'\n",
    "\n",
    "all_imgs = glob.glob(images + '/*.jpg',recursive=True)\n",
    "image_count = len(all_imgs)\n",
    "print(all_imgs[:3])\n",
    "print(\"The total images present in the dataset: {}\".format(image_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w8rS2jWX6gXJ",
    "outputId": "522ec93a-f82f-4627-ca01-249ed76361ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image,caption\n",
      "1000268201_693b08cb0e.jpg,A child in a pink dress is climbing up a set of stairs in an entry way .\n",
      "1000268201_693b08cb0e.jpg,A girl going into a wooden building .\n",
      "1000268201_693b08cb0e.jpg,A little girl climbing into a wooden playhouse .\n",
      "1000268201_693b08cb0e.jpg,A little girl climbing\n"
     ]
    }
   ],
   "source": [
    " #Import the dataset and read the text file into a seperate variable\n",
    "text_file = 'Flickr8K/captions.txt'\n",
    "def load_doc(filename):\n",
    "    #your code here\n",
    "    text = open(filename, 'r', encoding = 'utf-8').read()\n",
    "    return text\n",
    "\n",
    "doc = load_doc(text_file)\n",
    "print(doc[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fKqI69HDU8LT"
   },
   "outputs": [],
   "source": [
    "# all_imgs_t = glob.glob(images + '/*.jpg.npy',recursive=True)\n",
    "# image_count = len(all_imgs_t)\n",
    "# print(image_count)\n",
    "\n",
    "# for i in all_imgs_t:\n",
    "#     os.remove(i) \n",
    "\n",
    "# all_imgs_t = glob.glob(images + '/*.jpg.npy',recursive=True)\n",
    "# image_count = len(all_imgs_t)\n",
    "# print(image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "id": "VrGVRbsQ6gXK",
    "outputId": "b25e9865-4d60-4000-db51-df9de0a8cb3f"
   },
   "outputs": [],
   "source": [
    "# #Visualise both the images & text present in the dataset\n",
    "# for i in range(3, 10, 2):\n",
    "#     plt.figure()\n",
    "#     img = PIL.Image.open(all_imgs[i])\n",
    "\n",
    "# #     img = cv2.imread(all_imgs[i])\n",
    "# #     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#     plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQlsChsQ6gXL"
   },
   "source": [
    "Create a dataframe which summarizes the image, path & captions as a dataframe\n",
    "\n",
    "Each image id has 5 captions associated with it therefore the total dataset should have 40455 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "K2Gax6Qm6gXL"
   },
   "outputs": [],
   "source": [
    "# utility function to clean the caption text\n",
    "def clean_caption(caption):\n",
    "    table = str.maketrans('','', string.punctuation)\n",
    "    desc = caption.split() #create a list of words\n",
    "\n",
    "    # remove the punctuation from each word\n",
    "    desc = [word.translate(table) for word in desc]\n",
    "    \n",
    "    # remove empty strings and strings like 's and a\n",
    "    desc = [word.strip() for word in desc if len(word.strip())>1]\n",
    "    desc = [word.lower() for word in desc if(word.isalpha())] # convert to lower case and pick only alpha numeric \n",
    "    return ' '.join(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Gqv2_kN-6gXM"
   },
   "outputs": [],
   "source": [
    "# utility token to append <start> and <end>\n",
    "def appendDelimitingTokes(caption):\n",
    "    return f'<start> {caption} <end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6cczg2Zf6gXM"
   },
   "outputs": [],
   "source": [
    "# utility function to generate image path\n",
    "def getImagePaths(s):\n",
    "    return s.replace('\\\\', '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "5BzFlvqE6gXM"
   },
   "outputs": [],
   "source": [
    "def getFileName(path):\n",
    "    return path[len(images)+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lynUSp5g6gXM"
   },
   "outputs": [],
   "source": [
    "all_img_id= list(map(getFileName, all_imgs))#store all the image id here\n",
    "all_img_vector= list(map(getImagePaths, all_imgs)) #store all the image path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "weXfCmtR6gXN"
   },
   "outputs": [],
   "source": [
    "# utility function to generate mappings of captions to images as a dictionary with image name as index. \n",
    "def MapImageCaptions(doc):\n",
    "    captionDict = dict()\n",
    "    text_lines = doc.split('\\n')\n",
    "    # remove the heading line and empty lines\n",
    "    text_lines = [t for t in text_lines[1:] if len(t)>0]\n",
    "    for line in text_lines:\n",
    "        dict_index, dict_val = line.split(',', 1)\n",
    "        if dict_index not in captionDict:\n",
    "            captionDict[dict_index] = list()\n",
    "        captionDict[dict_index].append(dict_val)\n",
    "    return captionDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lx5e1tMM6gXN",
    "outputId": "77fa8cce-53a2-4d05-8bca-dd1324560b99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8091"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captionMappings = MapImageCaptions(doc)\n",
    "len(captionMappings.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "QJtW-MbU6gXO"
   },
   "outputs": [],
   "source": [
    "annotations= captionMappings.values() #store all the captions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "5MC_ZS3m6gXP",
    "outputId": "e2a73554-770f-4503-f326-7bf63dd268da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8091, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Path</th>\n",
       "      <th>Captions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2201222219_8d656b0633.jpg</td>\n",
       "      <td>Flickr8K/Images/2201222219_8d656b0633.jpg</td>\n",
       "      <td>[A child in a pink dress is climbing up a set ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3681172959_6674c118d2.jpg</td>\n",
       "      <td>Flickr8K/Images/3681172959_6674c118d2.jpg</td>\n",
       "      <td>[A black dog and a spotted dog are fighting, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3257107194_f235c8f7ab.jpg</td>\n",
       "      <td>Flickr8K/Images/3257107194_f235c8f7ab.jpg</td>\n",
       "      <td>[A little girl covered in paint sits in front ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2831656774_36982aafdb.jpg</td>\n",
       "      <td>Flickr8K/Images/2831656774_36982aafdb.jpg</td>\n",
       "      <td>[A man lays on a bench while his dog sits by h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3718892835_a3e74a3417.jpg</td>\n",
       "      <td>Flickr8K/Images/3718892835_a3e74a3417.jpg</td>\n",
       "      <td>[A man in an orange hat starring at something ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          ID                                       Path  \\\n",
       "0  2201222219_8d656b0633.jpg  Flickr8K/Images/2201222219_8d656b0633.jpg   \n",
       "1  3681172959_6674c118d2.jpg  Flickr8K/Images/3681172959_6674c118d2.jpg   \n",
       "2  3257107194_f235c8f7ab.jpg  Flickr8K/Images/3257107194_f235c8f7ab.jpg   \n",
       "3  2831656774_36982aafdb.jpg  Flickr8K/Images/2831656774_36982aafdb.jpg   \n",
       "4  3718892835_a3e74a3417.jpg  Flickr8K/Images/3718892835_a3e74a3417.jpg   \n",
       "\n",
       "                                            Captions  \n",
       "0  [A child in a pink dress is climbing up a set ...  \n",
       "1  [A black dog and a spotted dog are fighting, A...  \n",
       "2  [A little girl covered in paint sits in front ...  \n",
       "3  [A man lays on a bench while his dog sits by h...  \n",
       "4  [A man in an orange hat starring at something ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a data frame\n",
    "df = pd.DataFrame(list(zip(all_img_id, all_img_vector,annotations)),columns =['ID','Path', 'Captions']) \n",
    "print(df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "FNiIfTgD6gXP",
    "outputId": "a24811d9-442e-4c68-c334-14b826dab0ee"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Path</th>\n",
       "      <th>Captions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2201222219_8d656b0633.jpg</td>\n",
       "      <td>Flickr8K/Images/2201222219_8d656b0633.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2201222219_8d656b0633.jpg</td>\n",
       "      <td>Flickr8K/Images/2201222219_8d656b0633.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2201222219_8d656b0633.jpg</td>\n",
       "      <td>Flickr8K/Images/2201222219_8d656b0633.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2201222219_8d656b0633.jpg</td>\n",
       "      <td>Flickr8K/Images/2201222219_8d656b0633.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2201222219_8d656b0633.jpg</td>\n",
       "      <td>Flickr8K/Images/2201222219_8d656b0633.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          ID                                       Path  \\\n",
       "0  2201222219_8d656b0633.jpg  Flickr8K/Images/2201222219_8d656b0633.jpg   \n",
       "1  2201222219_8d656b0633.jpg  Flickr8K/Images/2201222219_8d656b0633.jpg   \n",
       "2  2201222219_8d656b0633.jpg  Flickr8K/Images/2201222219_8d656b0633.jpg   \n",
       "3  2201222219_8d656b0633.jpg  Flickr8K/Images/2201222219_8d656b0633.jpg   \n",
       "4  2201222219_8d656b0633.jpg  Flickr8K/Images/2201222219_8d656b0633.jpg   \n",
       "\n",
       "                                            Captions  \n",
       "0  A child in a pink dress is climbing up a set o...  \n",
       "1              A girl going into a wooden building .  \n",
       "2   A little girl climbing into a wooden playhouse .  \n",
       "3  A little girl climbing the stairs to her playh...  \n",
       "4  A little girl in a pink dress going into a woo...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (df\n",
    " .set_index(['ID','Path'])['Captions']\n",
    " .apply(pd.Series)\n",
    " .stack()\n",
    " .reset_index()\n",
    " .drop('level_2', axis=1)\n",
    " .rename(columns={0:'Captions'}))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ix9MDfEO6gXP"
   },
   "outputs": [],
   "source": [
    "# reference from  https://fairyonice.github.io/Develop_an_image_captioning_deep_learning_model_using_Flickr_8K_data.html\n",
    "def image_desc_plotter():\n",
    "    npic = 5\n",
    "    npix = 224\n",
    "    target_size = (npix,npix,3)\n",
    "\n",
    "    count = 1\n",
    "    fig = plt.figure(figsize=(10,20))\n",
    "    for image in all_img_vector[10:15]:\n",
    "        image_id = getFileName(image)\n",
    "        captions = list(df[\"Captions\"].loc[df[\"ID\"]==image_id].values)\n",
    "        image_load = load_img(image, target_size=target_size)\n",
    "\n",
    "        ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n",
    "        ax.imshow(image_load)\n",
    "        count += 1\n",
    "\n",
    "        ax = fig.add_subplot(npic,2,count)\n",
    "        plt.axis('off')\n",
    "        ax.plot()\n",
    "        ax.set_xlim(0,1)\n",
    "        ax.set_ylim(0,len(captions))\n",
    "        for i, caption in enumerate(captions):\n",
    "            ax.text(0,i,caption,fontsize=20)\n",
    "        count += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6-Ko9qFj6gXQ",
    "outputId": "f4f17063-6e01-4cd7-9a3c-ea7191ea1bb9"
   },
   "outputs": [],
   "source": [
    "# image_desc_plotter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bqIxQnvm6gXQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "91x9JM8b6gXQ"
   },
   "outputs": [],
   "source": [
    "## returns 2 lists first for clean annotations second for annotations appended with <start> and <end>\n",
    "def GetAllAnnotations(annotations):\n",
    "    allcaptions = list()\n",
    "    dCaptions = list()\n",
    "    for caption_list in annotations:\n",
    "        for caption in caption_list:\n",
    "            caption = clean_caption(caption)\n",
    "            dCaption = appendDelimitingTokes(caption)\n",
    "            allcaptions.append(caption)\n",
    "            dCaptions.append(dCaption)\n",
    "    return (allcaptions, dCaptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fje7qu5w6gXR",
    "outputId": "a47c856e-f9ac-4884-baef-4a289875c6a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total captions present in the dataset: 40455\n",
      "Total images present in the dataset: 8091\n",
      "First annotation child in pink dress is climbing up set of stairs in an entry way\n",
      "First delimited annotation <start> child in pink dress is climbing up set of stairs in an entry way <end>\n"
     ]
    }
   ],
   "source": [
    "#Create a list which contains all the captions\n",
    "#add the <start> & <end> token to all those captions as well\n",
    "annotations, delimitedAnnotations = GetAllAnnotations(annotations) #write your code here\n",
    "\n",
    "\n",
    "#Create a list which contains all the path to the images\n",
    "all_img_path= all_img_vector#write your code here\n",
    "\n",
    "print(\"Total captions present in the dataset: \"+ str(len(annotations)))\n",
    "print(\"Total images present in the dataset: \" + str(len(all_img_path)))\n",
    "print(\"First annotation \" + annotations[0])\n",
    "print(\"First delimited annotation \" + delimitedAnnotations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "biRF2Hv96gXS"
   },
   "outputs": [],
   "source": [
    "#Create the vocabulary & the counter for the captions\n",
    "\n",
    "vocabulary= [word for line in delimitedAnnotations for word in line.split()] #write your code here\n",
    "# val_count\n",
    "val_count=Counter(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "id": "p6j5hWY66gXT",
    "outputId": "76e847dd-4fd0-49a5-de35-c87d3be3e4f2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualise the top 30 occuring words in the captions\n",
    "#write your code here\n",
    "plt.figure(figsize=(20, 10))\n",
    "top_words = OrderedDict(val_count.most_common(30))\n",
    "plt.bar(top_words.keys(), top_words.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXazW4kK6gXX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0Ah_p8x6gXX"
   },
   "source": [
    "## Pre-Processing the captions\n",
    "1.Create the tokenized vectors by tokenizing the captions fore ex :split them using spaces & other filters. \n",
    "This gives us a vocabulary of all of the unique words in the data. Keep the total vocaublary to top 5,000 words for saving memory.\n",
    "\n",
    "2.Replace all other words with the unknown token \"UNK\" .\n",
    "\n",
    "3.Create word-to-index and index-to-word mappings.\n",
    "\n",
    "4.Pad all sequences to be the same length as the longest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "WpMxgZuN6gXX"
   },
   "outputs": [],
   "source": [
    "# create the tokenizer\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer(num_words = 5001 , oov_token='<unk>', filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(delimitedAnnotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "fwUEKXk56gXY"
   },
   "outputs": [],
   "source": [
    "# Create word-to-index and index-to-word mappings.\n",
    "# word to index mapping\n",
    "wordIndex = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "roojDGJe6gXZ"
   },
   "outputs": [],
   "source": [
    "#index-to-word mappings\n",
    "indexToWord = tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "BDuN2FrbJXrG"
   },
   "outputs": [],
   "source": [
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "rpnxTr956gXa",
    "outputId": "aeba48d4-09b8-4c11-ed66-557865dd847d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a word count of your tokenizer to visulize the Top 30 occuring words after text processing\n",
    "\n",
    "#your code here\n",
    "wordCounts = tokenizer.word_counts\n",
    "plt.figure(figsize=(20, 8))\n",
    "top_words = OrderedDict(Counter(wordCounts).most_common(30))\n",
    "plt.bar(top_words.keys(), top_words.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "A-wCf3pk6gXa"
   },
   "outputs": [],
   "source": [
    "text_sequences = tokenizer.texts_to_sequences(delimitedAnnotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "APwvHUFq6gXa",
    "outputId": "c9320482-9884-4d0a-e7c4-96ec077544fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MaxLength = max(len(d) for d in text_sequences)\n",
    "MaxLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1HGNtwII6gXb",
    "outputId": "4487e42d-b9ca-46ea-c5fb-5d1de859cf4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of Caption vector is :(40455, 34)\n"
     ]
    }
   ],
   "source": [
    "# Pad each vector to the max_length of the captions ^ store it to a vairable\n",
    "cap_vector= pad_sequences(text_sequences, maxlen=MaxLength, padding='post') #your code here\n",
    "print(\"The shape of Caption vector is :\" + str(cap_vector.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "OC3AQp_66gXc"
   },
   "outputs": [],
   "source": [
    "df['Captions'] = df['Captions'].apply(lambda x:appendDelimitingTokes(clean_caption(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drMe7_6d6gXc"
   },
   "source": [
    "## Pre-processing the images\n",
    "\n",
    "1.Resize them into the shape of (299, 299)\n",
    "\n",
    "3.Normalize the image within the range of -1 to 1, such that it is in correct format for InceptionV3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "GZYDjKoJ6gXd"
   },
   "outputs": [],
   "source": [
    "#write your code here\n",
    "# define the image size\n",
    "rows = 299\n",
    "cols = 299\n",
    "\n",
    "def get_encoded_label(label):\n",
    "    text_sequences = tokenizer.texts_to_sequences([label])\n",
    "    return pad_sequences(text_sequences, maxlen=MaxLength, padding='post')[0]\n",
    "\n",
    "# define a function to resize the images on demand.\n",
    "def resize_image(image, height=rows, width=cols):\n",
    "    img = tf.image.decode_jpeg(image, channels=3)\n",
    "    # resize the image to the desired size\n",
    "    img = tf.image.resize(img, [height, width])\n",
    "    return tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "\n",
    "# define a function to normalize the images on demand between -1 and 1\n",
    "def normalize_image(image):\n",
    "    normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset=-1)\n",
    "    return normalization_layer(image)\n",
    "    \n",
    "# utility function to read image from given path and process it for inception model\n",
    "def process_path(file_path):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, file_path\n",
    "\n",
    "def load_image_amit(file_path, cap=None):\n",
    "    img , _ = process_path(file_path)\n",
    "    return img, cap\n",
    "\n",
    "# function to load feature vector saved using imagenet earlier and map it to caption\n",
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    return img_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Kp8VF5EpDmZf"
   },
   "outputs": [],
   "source": [
    "batch_size=30\n",
    "BUFFER_SIZE = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lRMqqMb6gXf"
   },
   "source": [
    "## Load the pretrained Imagenet weights of Inception net V3\n",
    "\n",
    "1.To save the memory(RAM) from getting exhausted, extract the features of thei mage using the last layer of pre-trained model. Including this as part of training will lead to higher computational time.\n",
    "\n",
    "2.The shape of the output of this layer is 8x8x2048. \n",
    "\n",
    "3.Use a function to extract the features of each image in the train & test dataset such that the shape of each image should be (batch_size, 8*8, 2048)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LEObzOvu6gXg",
    "outputId": "f2b8b00f-4fe0-4ff6-97bc-2e586a1ed408"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top=False,weights='imagenet')\n",
    "\n",
    "new_input = image_model.input  #write code here to get the input of the image_model\n",
    "hidden_layer = image_model.layers[-1].output #write code here to get the output of the image_model\n",
    "\n",
    "image_features_extract_model =  tf.keras.Model(new_input, hidden_layer) #build the final model using both input & output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "HOTuTFia6gXg"
   },
   "outputs": [],
   "source": [
    "# write your code to extract features from each image in the dataset\n",
    "image_vector = sorted(set(all_img_vector))\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(image_vector)\n",
    "image_dataset = image_dataset.map(process_path, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(batch_size)\n",
    "# for img, path in image_dataset:\n",
    "#     batch_features = image_features_extract_model(img)\n",
    "#     batch_features = tf.reshape(batch_features,\n",
    "#                               (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "\n",
    "#     # we save the features to disk. \n",
    "#     for bf, p in zip(batch_features, path):\n",
    "#         path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "#         np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYhxCwyMDmZh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKg2DFLD6gXd"
   },
   "source": [
    "## Create the train & test data \n",
    "1.Combine both images & captions to create the train & test dataset using tf.data.Dataset API. Create the train-test spliit using 80-20 ratio & random state = 42\n",
    "\n",
    "2.Make sure you have done Shuffle and batch while building the dataset\n",
    "\n",
    "3.The shape of each image in the dataset after building should be (batch_size, 299, 299, 3)\n",
    "\n",
    "4.The shape of each caption in the dataset after building should be(batch_size, max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "UvMfySO96gXd"
   },
   "outputs": [],
   "source": [
    "# get data from dataframe based on keys\n",
    "def getKeyValueLists(keys):\n",
    "    data = df.loc[df['ID'].isin(keys)]\n",
    "    return data['Path'].tolist(), data['Captions'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OiTnMT-06gXd",
    "outputId": "c0a20ed1-6d93-4be4-be45-1dc4e61180c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6472, 1619)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images, val_images = train_test_split(all_img_id, train_size=0.8, random_state = 42)\n",
    "len(train_images),len(val_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qN1jeLMz6gXe",
    "outputId": "2a38931f-18c3-458f-8b2b-b3abc10f4212"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32360 32360\n"
     ]
    }
   ],
   "source": [
    "train_image_idx, train_captions = getKeyValueLists(train_images)\n",
    "print(len(train_image_idx), len(train_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0iFEzvaS6gXe",
    "outputId": "b44a985f-6c92-4cf5-fe5d-00bc4e7f913b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8095 8095\n"
     ]
    }
   ],
   "source": [
    "val_image_idx, val_captions = getKeyValueLists(val_images)\n",
    "print(len(val_image_idx), len(val_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dqRRlVhU6gXe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "n-mnWr7o6gXg"
   },
   "outputs": [],
   "source": [
    "## tokenize the captions\n",
    "train_captions =tokenizer.texts_to_sequences( train_captions)\n",
    "train_captions = pad_sequences(train_captions, maxlen=MaxLength, padding='post')\n",
    "\n",
    "\n",
    "val_captions =tokenizer.texts_to_sequences( val_captions)\n",
    "val_captions = pad_sequences(val_captions, maxlen=MaxLength, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "78EamtSw6gXg"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_image_idx, train_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "A6VFGiAjJXrM"
   },
   "outputs": [],
   "source": [
    "# Use map to load the numpy files in parallel\n",
    "train_dataset = train_dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          load_image_amit, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XuaOx7Jt6gXf",
    "outputId": "30bd031d-7462-4a87-81bb-50ef95428473"
   },
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": "InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run RealDiv: Attempted to set tensor for existing mirror. [Op:RealDiv]\nTraceback (most recent call last):\n\n  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 243, in __call__\n    ret = func(*args)\n\n  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 309, in wrapper\n    return func(*args, **kwargs)\n\n  File \"<ipython-input-31-f2761b5d1afb>\", line 31, in load_image_amit\n    img , _ = process_path(file_path)\n\n  File \"<ipython-input-31-f2761b5d1afb>\", line 27, in process_path\n    img = tf.keras.applications.inception_v3.preprocess_input(img)\n\n  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/applications/inception_v3.py\", line 420, in preprocess_input\n    return imagenet_utils.preprocess_input(x, data_format=data_format, mode='tf')\n\n  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/applications/imagenet_utils.py\", line 110, in preprocess_input\n    x, data_format=data_format, mode=mode)\n\n  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/applications/imagenet_utils.py\", line 253, in _preprocess_symbolic_input\n    x /= 127.5\n\n  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 997, in binary_op_wrapper\n    return func(x, y, name=name)\n\n  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 1090, in _truediv_python3\n    return gen_math_ops.real_div(x, y, name=name)\n\n  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 7264, in real_div\n    _ops.raise_from_not_ok_status(e, name)\n\n  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 6653, in raise_from_not_ok_status\n    six.raise_from(core._status_to_exception(e.code, message), None)\n\n  File \"<string>\", line 3, in raise_from\n\ntensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run RealDiv: Attempted to set tensor for existing mirror. [Op:RealDiv]\n\n\n\t [[{{node PyFunc}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   1985\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m       \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6652\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6653\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6654\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m: InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run RealDiv: Attempted to set tensor for existing mirror. [Op:RealDiv]\nTraceback (most recent call last):\n\n  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 243, in __call__\n    ret = func(*args)\n\n  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 309, in wrapper\n    return func(*args, **kwargs)\n\n  File \"<ipython-input-31-f2761b5d1afb>\", line 31, in load_image_amit\n    img , _ = process_path(file_path)\n\n  File \"<ipython-input-31-f2761b5d1afb>\", line 27, in process_path\n    img = tf.keras.applications.inception_v3.preprocess_input(img)\n\n  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/applications/inception_v3.py\", line 420, in preprocess_input\n    return imagenet_utils.preprocess_input(x, data_format=data_format, mode='tf')\n\n  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/applications/imagenet_utils.py\", line 110, in preprocess_input\n    x, data_format=data_format, mode=mode)\n\n  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/applications/imagenet_utils.py\", line 253, in _preprocess_symbolic_input\n    x /= 127.5\n\n  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 997, in binary_op_wrapper\n    return func(x, y, name=name)\n\n  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 1090, in _truediv_python3\n    return gen_math_ops.real_div(x, y, name=name)\n\n  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 7264, in real_div\n    _ops.raise_from_not_ok_status(e, name)\n\n  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 6653, in raise_from_not_ok_status\n    six.raise_from(core._status_to_exception(e.code, message), None)\n\n  File \"<string>\", line 3, in raise_from\n\ntensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run RealDiv: Attempted to set tensor for existing mirror. [Op:RealDiv]\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-f5e5e0e2422f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample_img_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_cap_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_img_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(batch_size, 299, 299, 3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_cap_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(batch_size, max_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[0;34m\"\"\"Returns a nested structure of `Tensor`s containing the next element.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   1987\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1989\u001b[0;31m       \u001b[0mexecutor_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/executor.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;34m\"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_ExecutorWaitForAllPendingNodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mclear_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m: InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run RealDiv: Attempted to set tensor for existing mirror. [Op:RealDiv]\nTraceback (most recent call last):\n\n  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 243, in __call__\n    ret = func(*args)\n\n  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 309, in wrapper\n    return func(*args, **kwargs)\n\n  File \"<ipython-input-31-f2761b5d1afb>\", line 31, in load_image_amit\n    img , _ = process_path(file_path)\n\n  File \"<ipython-input-31-f2761b5d1afb>\", line 27, in process_path\n    img = tf.keras.applications.inception_v3.preprocess_input(img)\n\n  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/applications/inception_v3.py\", line 420, in preprocess_input\n    return imagenet_utils.preprocess_input(x, data_format=data_format, mode='tf')\n\n  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/applications/imagenet_utils.py\", line 110, in preprocess_input\n    x, data_format=data_format, mode=mode)\n\n  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/applications/imagenet_utils.py\", line 253, in _preprocess_symbolic_input\n    x /= 127.5\n\n  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 997, in binary_op_wrapper\n    return func(x, y, name=name)\n\n  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 1090, in _truediv_python3\n    return gen_math_ops.real_div(x, y, name=name)\n\n  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 7264, in real_div\n    _ops.raise_from_not_ok_status(e, name)\n\n  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 6653, in raise_from_not_ok_status\n    six.raise_from(core._status_to_exception(e.code, message), None)\n\n  File \"<string>\", line 3, in raise_from\n\ntensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run RealDiv: Attempted to set tensor for existing mirror. [Op:RealDiv]\n\n\n\t [[{{node PyFunc}}]]"
     ]
    }
   ],
   "source": [
    "sample_img_batch, sample_cap_batch = next(iter(train_dataset))\n",
    "print(sample_img_batch.shape) #(batch_size, 299, 299, 3)\n",
    "print(sample_cap_batch.shape) #(batch_size, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itsOP9BKyTzU"
   },
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SDrOBVgxJXrM"
   },
   "outputs": [],
   "source": [
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((val_image_idx, val_captions))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "test_dataset = test_dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          load_image_amit, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p5uw6I_-yTzV",
    "outputId": "9d1cc06c-0aac-4412-b321-54c9f0ab59d7"
   },
   "outputs": [],
   "source": [
    "sample_img_batch_test, sample_cap_batch_test = next(iter(test_dataset))\n",
    "print(sample_img_batch_test.shape) #(batch_size, 299, 299, 3)\n",
    "print(sample_cap_batch_test.shape) #(batch_size, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFpGFfKyDmZm"
   },
   "source": [
    "## Train data from feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EnhcpGEt6gXh"
   },
   "outputs": [],
   "source": [
    "#  create a tensor from given data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_image_idx, train_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FN-tFAM6gXh"
   },
   "outputs": [],
   "source": [
    "# Use map to load the numpy files in parallel\n",
    "train_dataset = train_dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cl4Yznx96gXh"
   },
   "outputs": [],
   "source": [
    "# Shuffle and batch\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(batch_size, drop_remainder=True)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wvcKn3gJ6gXh",
    "outputId": "b2a1fe1d-9ff2-4bc0-cb19-64e04d6ecdf2"
   },
   "outputs": [],
   "source": [
    "sample_img_batch, sample_cap_batch = next(iter(train_dataset))\n",
    "print(sample_img_batch.shape)  #(batch_size, 8*8, 2048)\n",
    "print(sample_cap_batch.shape) #(batch_size,40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSR-TM5kyTzY"
   },
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G4ZONgzAyTzY"
   },
   "outputs": [],
   "source": [
    "#  create a tensor from given data\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((val_image_idx, val_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKUqbjiGyTzY"
   },
   "outputs": [],
   "source": [
    "# Use map to load the numpy files in parallel\n",
    "test_dataset = test_dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6AOWjwH6yTzZ"
   },
   "outputs": [],
   "source": [
    "# Shuffle and batch\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(batch_size,drop_remainder=True)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m5PEetDyyTzZ",
    "outputId": "6d4df2e7-f7dd-4c67-fd7f-8490153d3f04"
   },
   "outputs": [],
   "source": [
    "sample_img_batch_test, sample_cap_batch_test = next(iter(test_dataset))\n",
    "print(sample_img_batch_test.shape)  #(batch_size, 8*8, 2048)\n",
    "print(sample_cap_batch_test.shape) #(batch_size,40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkO6bpzL6gXi"
   },
   "source": [
    "## Model Building\n",
    "1.Set the parameters\n",
    "\n",
    "2.Build the Encoder, Attention model & Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PIHD8AL_6gXi"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 256 \n",
    "units = 512\n",
    "vocab_size = 5001 #top 5,000 words +1\n",
    "train_num_steps = len(train_image_idx) // batch_size\n",
    "test_num_steps = len(val_image_idx) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CaFhLOVb6gXi"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b326M1fm6gXi"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,embed_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dense = tf.keras.layers.Dense(embed_dim)#build your Dense layer with relu activation\n",
    "        \n",
    "    def call(self, features):\n",
    "        features =  self.dense(features) # extract the features from the image shape: (batch, 8*8, embed_dim)\n",
    "        features = tf.nn.relu(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Iugu5xM6gXj"
   },
   "outputs": [],
   "source": [
    "encoder=Encoder(embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJZauLck6gXj"
   },
   "source": [
    "### Attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l6Rcosn46gXj"
   },
   "outputs": [],
   "source": [
    "class Attention_model(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(Attention_model, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units) #build your Dense layer\n",
    "        self.W2 = tf.keras.layers.Dense(units) #build your Dense layer\n",
    "        self.V = tf.keras.layers.Dense(1) #build your final Dense layer with unit 1\n",
    "        self.units=units\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        #features shape: (batch_size, 8*8, embedding_dim)\n",
    "        # hidden shape: (batch_size, hidden_size)\n",
    "        hidden_with_time_axis =  tf.expand_dims(hidden, 1)# Expand the hidden shape to shape: (batch_size, 1, hidden_size)\n",
    "        attention_hidden_layer = (tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        score = self.V(attention_hidden_layer) # build your score funciton to shape: (batch_size, 8*8, units)\n",
    "        attention_weights =  tf.nn.softmax(score, axis=1) # extract your attention weights with shape: (batch_size, 8*8, 1)\n",
    "        context_vector =  attention_weights * features #shape: create the context vector with shape (batch_size, 8*8,embedding_dim)\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1) # reduce the shape to (batch_size, embedding_dim)\n",
    "        \n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roSekRIX6gXk"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mTbPha296gXk"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, embed_dim, units, vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.units=units\n",
    "        self.attention = Attention_model(self.units)#iniitalise your Attention model with units\n",
    "        self.embed = tf.keras.layers.Embedding(vocab_size, embedding_dim) #build your Embedding layer\n",
    "        self.gru = tf.keras.layers.GRU(self.units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')\n",
    "        self.d1 = tf.keras.layers.Dense(self.units) #build your Dense layer\n",
    "        self.d2 = tf.keras.layers.Dense(vocab_size) #build your Dense layer\n",
    "        \n",
    "\n",
    "    def call(self,x,features, hidden):\n",
    "        context_vector, attention_weights = self.attention(features, hidden) #create your context vector & attention weights from attention model\n",
    "        embed = self.embed(x)# embed your input to shape: (batch_size, 1, embedding_dim)\n",
    "        embed = tf.concat([tf.expand_dims(context_vector, 1), embed], axis=-1)  # Concatenate your input with the context vector from attention layer. Shape: (batch_size, 1, embedding_dim + embedding_dim)\n",
    "        output,state = self.gru(embed)# Extract the output & hidden state from GRU layer. Output shape : (batch_size, max_length, hidden_size)\n",
    "        output = self.d1(output)\n",
    "        output = tf.reshape(output, (-1, output.shape[2])) # shape : (batch_size * max_length, hidden_size)\n",
    "        output = self.d2(output) # shape : (batch_size * max_length, vocab_size)\n",
    "        \n",
    "        return output,state, attention_weights\n",
    "    # reset_state\n",
    "    def init_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UODeFhc06gXk"
   },
   "outputs": [],
   "source": [
    "decoder=Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hhf1dhr46gXk"
   },
   "outputs": [],
   "source": [
    "features=encoder(sample_img_batch)\n",
    "\n",
    "hidden = decoder.init_state(batch_size=sample_cap_batch.shape[0])\n",
    "dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * sample_cap_batch.shape[0], 1)\n",
    "dec_input = tf.cast(dec_input, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-iMFsUfvRGNB",
    "outputId": "128bd250-11d3-4424-edbb-dc584d9fa408"
   },
   "outputs": [],
   "source": [
    "predictions, hidden_out, attention_weights= decoder(dec_input, features, hidden)\n",
    "print('Feature shape from Encoder: {}'.format(features.shape)) #(batch, 8*8, embed_dim)\n",
    "print('Predcitions shape from Decoder: {}'.format(predictions.shape)) #(batch,vocab_size)\n",
    "print('Attention weights shape from Decoder: {}'.format(attention_weights.shape)) #(batch, 8*8, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwU-iwqq6gXk"
   },
   "source": [
    "## Model training & optimization\n",
    "1.Set the optimizer & loss object\n",
    "\n",
    "2.Create your checkpoint path\n",
    "\n",
    "3.Create your training & testing step functions\n",
    "\n",
    "4.Create your loss function for the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apt9U1Xf6gXl"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam() #define the optimizer\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')#define your loss object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBTRxrgt6gXl"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4XI40SRq6gXl"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGreMsEQ6gXl"
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "# if ckpt_manager.latest_checkpoint:\n",
    "#     start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "#     # restoring the latest checkpoint in checkpoint_path\n",
    "#     ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cnNELKW96gXm"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "    hidden = decoder.init_state(batch_size=target.shape[0])\n",
    "    # print(hidden.shape)\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "    # print(dec_input.shape)\n",
    "    with tf.GradientTape() as tape:\n",
    "        #write your code here to do the training steps\n",
    "        feature = encoder(img_tensor)\n",
    "        # print(feature.shape)\n",
    "        for i in range(1, target.shape[1]):\n",
    "            predictions, hidden, _ = decoder(dec_input, feature, hidden)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "            \n",
    "    avg_loss = (loss / int(target.shape[1]))\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39zwHq5G6gXm"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(img_tensor, target):\n",
    "    loss = 0\n",
    "    #write your code here to do the testing steps\n",
    "    hidden = decoder.init_state(batch_size=target.shape[0])\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "    features = encoder(img_tensor)\n",
    "    for i in range(1, target.shape[1]):\n",
    "    # passing the features through the decoder\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "        # predictions : (64,8329)\n",
    "        loss += loss_function(target[:, i], predictions)\n",
    "      \n",
    "        predicted_id = tf.argmax(predictions[0])\n",
    "        dec_input = tf.expand_dims([predicted_id]*batch_size, 1)\n",
    " \n",
    "    avg_loss = (loss / int(target.shape[1]))\n",
    "        \n",
    "    return loss, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jb64chUm6gXm"
   },
   "outputs": [],
   "source": [
    "def test_loss_cal(test_dataset):\n",
    "    total_loss = 0\n",
    "    #write your code to get the average loss result on your test data\n",
    "    # i=0\n",
    "    for (batch, (img_tensor, target)) in enumerate(test_dataset):\n",
    "        print(batch, img_tensor.shape, target.shape)\n",
    "        batch_loss, t_loss = test_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "        # i= i+1\n",
    "        # if i>2:\n",
    "        #   break\n",
    "\n",
    "    avg_test_loss = total_loss/test_num_steps\n",
    "    return avg_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZg8xF-sq299"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "loss_plot = []\n",
    "test_loss_plot = []\n",
    "best_test_loss=100\n",
    "start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gOyquoCI6gXm",
    "outputId": "240357b3-cd6a-4fb8-b8c6-256757cd75de"
   },
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / train_num_steps)\n",
    "    \n",
    "    test_loss = test_loss_cal(test_dataset)\n",
    "    test_loss_plot.append(test_loss)\n",
    "    \n",
    "    # print ('For epoch: {}, the train loss is {:.3f}, & test loss is {:.3f}'.format(epoch+1,avg_train_loss,test_loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    \n",
    "    if test_loss < best_test_loss:\n",
    "        print('Test loss has been reduced from %.3f to %.3f' % (best_test_loss, test_loss))\n",
    "        best_test_loss = test_loss\n",
    "        ckpt_manager.save()\n",
    "        \n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,total_loss/train_num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "Co4C0jIA6gXn",
    "outputId": "fbdcbb20-8f2b-4df4-e709-04e0956e0a62"
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.plot(test_loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhbFHrf-6gXn"
   },
   "source": [
    "## Model Evaluation\n",
    "1.Define your evaluation function using greedy search\n",
    "\n",
    "2.Define your evaluation function using beam search ( optional)\n",
    "\n",
    "3.Test it on a sample data using BLEU score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69mqfSv-6gXr"
   },
   "source": [
    "### Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DF6rMqZd6gXs"
   },
   "outputs": [],
   "source": [
    "attention_features_shape = 64 #assign from relevant variable above\n",
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((MaxLength, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.init_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(process_path(image)[0], 0) #process the input image to desired format before extracting features\n",
    "    img_tensor_val = image_features_extract_model(temp_input) # Extract features using our feature extraction model\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)# extract the features by passing the input to encoder\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(MaxLength):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, feature, hidden)# get the output from decoder\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()#extract the predicted id(embedded value) which carries the max value\n",
    "        \n",
    "        #map the id to the word from tokenizer and append the value to the result list\n",
    "        predicted_word = tokenizer.index_word[predicted_id]\n",
    "        result.append(predicted_word)\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot,predictions\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot,predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlmm8YVl6gXs"
   },
   "source": [
    "### Beam Search(optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UspPZ0hn6gXt"
   },
   "outputs": [],
   "source": [
    "def beam_evaluate(image, beam_index =7):\n",
    "    # captions on the validation set\n",
    "    start = time.time()\n",
    "    result, attention_plot = evaluate(image)\n",
    "    #remove \"<unk>\" in result\n",
    "    for i in result:\n",
    "        if i==\"<unk>\":\n",
    "            result.remove(i)\n",
    "\n",
    "\n",
    "    #remove <end> from result         \n",
    "    result_join = ' '.join(result)\n",
    "    final_caption = result_join.rsplit(' ', 1)[0]\n",
    "    print ('Prediction Caption:', final_caption)\n",
    "\n",
    "    plot_attention(image, result, attention_plot)\n",
    "\n",
    "    print(f\"time took to Predict: {round(time.time()-start)} sec\")\n",
    "                  \n",
    "    return final_caption\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oWPZlukL6gXt"
   },
   "outputs": [],
   "source": [
    "def plot_attmap(caption, weights, image):\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    temp_img = np.array(Image.open(image))\n",
    "    \n",
    "    len_cap = len(caption)\n",
    "    for cap in range(len_cap):\n",
    "        weights_img = np.reshape(weights[cap], (8,8))\n",
    "        weights_img = np.array(Image.fromarray(weights_img).resize((224, 224), Image.LANCZOS))\n",
    "        \n",
    "        ax = fig.add_subplot(len_cap//2, len_cap//2, cap+1)\n",
    "        ax.set_title(caption[cap], fontsize=15)\n",
    "        \n",
    "        img=ax.imshow(temp_img)\n",
    "        \n",
    "        ax.imshow(weights_img, cmap='gist_heat', alpha=0.6,extent=img.get_extent())\n",
    "        ax.axis('off')\n",
    "    plt.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sV33b63y6gXt"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NWUGDejm6gXt"
   },
   "outputs": [],
   "source": [
    "def filt_text(text):\n",
    "    filt=['<start>','<unk>','<end>'] \n",
    "    temp= text.split()\n",
    "    [temp.remove(j) for k in filt for j in temp if k==j]\n",
    "    text=' '.join(temp)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ZRSp7jo6gXu"
   },
   "outputs": [],
   "source": [
    "rid = np.random.randint(0, len(val_image_idx))\n",
    "test_image = val_image_idx[rid]\n",
    "#test_image = './images/413231421_43833a11f5.jpg'\n",
    "#real_caption = '<start> black dog is digging in the snow <end>'\n",
    "\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in val_captions[rid] if i not in [0]])\n",
    "result, attention_plot,pred_test = evaluate(test_image)\n",
    "\n",
    "\n",
    "real_caption=filt_text(real_caption)      \n",
    "\n",
    "\n",
    "pred_caption=' '.join(result).rsplit(' ', 1)[0]\n",
    "\n",
    "real_appn = []\n",
    "real_appn.append(real_caption.split())\n",
    "reference = real_appn\n",
    "candidate = pred_caption.split()\n",
    "\n",
    "score = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0))#set your weights)\n",
    "print(f\"BELU score: {score*100}\")\n",
    "\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption:', pred_caption)\n",
    "plot_attmap(result, attention_plot, test_image)\n",
    "\n",
    "\n",
    "Image.open(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otO3iUmB6gXu"
   },
   "outputs": [],
   "source": [
    "captions=beam_evaluate(test_image)\n",
    "print(captions)\n",
    "\n",
    "\n",
    "candidate = captions.split()\n",
    "\n",
    "score = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0))\n",
    "print(f\"BELU score: {score*100}\")\n",
    "\n",
    "#print ('Real Caption:', real_caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2oah3PPZDmZx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kXuMdecpDmZx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Eye+for+blind+Starter+code-Copy1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
