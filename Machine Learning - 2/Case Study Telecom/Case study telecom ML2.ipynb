{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps performed on case study\n",
    "- Loading data set\n",
    "- Data cleaning\n",
    "- Imputing null values\n",
    "- Filtering high value customers\n",
    "- Deriving target variable\n",
    "- New predictor variable derivation\n",
    "- EDA\n",
    "- Data preparation\n",
    "- Logistic Regression\n",
    "- PCA\n",
    "- Decision tree\n",
    "- Random forest\n",
    "- XGBoost\n",
    "- Feature importance and business conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 300)\n",
    "pd.set_option(\"display.max_rows\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve ,confusion_matrix , precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "churn = pd.read_csv(\"telecom_churn_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mobile_number</th>\n",
       "      <th>circle_id</th>\n",
       "      <th>loc_og_t2o_mou</th>\n",
       "      <th>std_og_t2o_mou</th>\n",
       "      <th>loc_ic_t2o_mou</th>\n",
       "      <th>last_date_of_month_6</th>\n",
       "      <th>last_date_of_month_7</th>\n",
       "      <th>last_date_of_month_8</th>\n",
       "      <th>last_date_of_month_9</th>\n",
       "      <th>arpu_6</th>\n",
       "      <th>arpu_7</th>\n",
       "      <th>arpu_8</th>\n",
       "      <th>arpu_9</th>\n",
       "      <th>onnet_mou_6</th>\n",
       "      <th>onnet_mou_7</th>\n",
       "      <th>onnet_mou_8</th>\n",
       "      <th>onnet_mou_9</th>\n",
       "      <th>offnet_mou_6</th>\n",
       "      <th>offnet_mou_7</th>\n",
       "      <th>offnet_mou_8</th>\n",
       "      <th>offnet_mou_9</th>\n",
       "      <th>roam_ic_mou_6</th>\n",
       "      <th>roam_ic_mou_7</th>\n",
       "      <th>roam_ic_mou_8</th>\n",
       "      <th>roam_ic_mou_9</th>\n",
       "      <th>roam_og_mou_6</th>\n",
       "      <th>roam_og_mou_7</th>\n",
       "      <th>roam_og_mou_8</th>\n",
       "      <th>roam_og_mou_9</th>\n",
       "      <th>loc_og_t2t_mou_6</th>\n",
       "      <th>loc_og_t2t_mou_7</th>\n",
       "      <th>loc_og_t2t_mou_8</th>\n",
       "      <th>loc_og_t2t_mou_9</th>\n",
       "      <th>loc_og_t2m_mou_6</th>\n",
       "      <th>loc_og_t2m_mou_7</th>\n",
       "      <th>loc_og_t2m_mou_8</th>\n",
       "      <th>loc_og_t2m_mou_9</th>\n",
       "      <th>loc_og_t2f_mou_6</th>\n",
       "      <th>loc_og_t2f_mou_7</th>\n",
       "      <th>loc_og_t2f_mou_8</th>\n",
       "      <th>loc_og_t2f_mou_9</th>\n",
       "      <th>loc_og_t2c_mou_6</th>\n",
       "      <th>loc_og_t2c_mou_7</th>\n",
       "      <th>loc_og_t2c_mou_8</th>\n",
       "      <th>loc_og_t2c_mou_9</th>\n",
       "      <th>loc_og_mou_6</th>\n",
       "      <th>loc_og_mou_7</th>\n",
       "      <th>loc_og_mou_8</th>\n",
       "      <th>loc_og_mou_9</th>\n",
       "      <th>std_og_t2t_mou_6</th>\n",
       "      <th>std_og_t2t_mou_7</th>\n",
       "      <th>std_og_t2t_mou_8</th>\n",
       "      <th>std_og_t2t_mou_9</th>\n",
       "      <th>std_og_t2m_mou_6</th>\n",
       "      <th>std_og_t2m_mou_7</th>\n",
       "      <th>std_og_t2m_mou_8</th>\n",
       "      <th>std_og_t2m_mou_9</th>\n",
       "      <th>std_og_t2f_mou_6</th>\n",
       "      <th>std_og_t2f_mou_7</th>\n",
       "      <th>std_og_t2f_mou_8</th>\n",
       "      <th>std_og_t2f_mou_9</th>\n",
       "      <th>std_og_t2c_mou_6</th>\n",
       "      <th>std_og_t2c_mou_7</th>\n",
       "      <th>std_og_t2c_mou_8</th>\n",
       "      <th>std_og_t2c_mou_9</th>\n",
       "      <th>std_og_mou_6</th>\n",
       "      <th>std_og_mou_7</th>\n",
       "      <th>std_og_mou_8</th>\n",
       "      <th>std_og_mou_9</th>\n",
       "      <th>isd_og_mou_6</th>\n",
       "      <th>isd_og_mou_7</th>\n",
       "      <th>isd_og_mou_8</th>\n",
       "      <th>isd_og_mou_9</th>\n",
       "      <th>spl_og_mou_6</th>\n",
       "      <th>spl_og_mou_7</th>\n",
       "      <th>spl_og_mou_8</th>\n",
       "      <th>spl_og_mou_9</th>\n",
       "      <th>og_others_6</th>\n",
       "      <th>og_others_7</th>\n",
       "      <th>og_others_8</th>\n",
       "      <th>og_others_9</th>\n",
       "      <th>total_og_mou_6</th>\n",
       "      <th>total_og_mou_7</th>\n",
       "      <th>total_og_mou_8</th>\n",
       "      <th>total_og_mou_9</th>\n",
       "      <th>loc_ic_t2t_mou_6</th>\n",
       "      <th>loc_ic_t2t_mou_7</th>\n",
       "      <th>loc_ic_t2t_mou_8</th>\n",
       "      <th>loc_ic_t2t_mou_9</th>\n",
       "      <th>loc_ic_t2m_mou_6</th>\n",
       "      <th>loc_ic_t2m_mou_7</th>\n",
       "      <th>loc_ic_t2m_mou_8</th>\n",
       "      <th>loc_ic_t2m_mou_9</th>\n",
       "      <th>loc_ic_t2f_mou_6</th>\n",
       "      <th>loc_ic_t2f_mou_7</th>\n",
       "      <th>loc_ic_t2f_mou_8</th>\n",
       "      <th>loc_ic_t2f_mou_9</th>\n",
       "      <th>loc_ic_mou_6</th>\n",
       "      <th>loc_ic_mou_7</th>\n",
       "      <th>loc_ic_mou_8</th>\n",
       "      <th>loc_ic_mou_9</th>\n",
       "      <th>std_ic_t2t_mou_6</th>\n",
       "      <th>std_ic_t2t_mou_7</th>\n",
       "      <th>std_ic_t2t_mou_8</th>\n",
       "      <th>std_ic_t2t_mou_9</th>\n",
       "      <th>std_ic_t2m_mou_6</th>\n",
       "      <th>std_ic_t2m_mou_7</th>\n",
       "      <th>std_ic_t2m_mou_8</th>\n",
       "      <th>std_ic_t2m_mou_9</th>\n",
       "      <th>std_ic_t2f_mou_6</th>\n",
       "      <th>std_ic_t2f_mou_7</th>\n",
       "      <th>std_ic_t2f_mou_8</th>\n",
       "      <th>std_ic_t2f_mou_9</th>\n",
       "      <th>std_ic_t2o_mou_6</th>\n",
       "      <th>std_ic_t2o_mou_7</th>\n",
       "      <th>std_ic_t2o_mou_8</th>\n",
       "      <th>std_ic_t2o_mou_9</th>\n",
       "      <th>std_ic_mou_6</th>\n",
       "      <th>std_ic_mou_7</th>\n",
       "      <th>std_ic_mou_8</th>\n",
       "      <th>std_ic_mou_9</th>\n",
       "      <th>total_ic_mou_6</th>\n",
       "      <th>total_ic_mou_7</th>\n",
       "      <th>total_ic_mou_8</th>\n",
       "      <th>total_ic_mou_9</th>\n",
       "      <th>spl_ic_mou_6</th>\n",
       "      <th>spl_ic_mou_7</th>\n",
       "      <th>spl_ic_mou_8</th>\n",
       "      <th>spl_ic_mou_9</th>\n",
       "      <th>isd_ic_mou_6</th>\n",
       "      <th>isd_ic_mou_7</th>\n",
       "      <th>isd_ic_mou_8</th>\n",
       "      <th>isd_ic_mou_9</th>\n",
       "      <th>ic_others_6</th>\n",
       "      <th>ic_others_7</th>\n",
       "      <th>ic_others_8</th>\n",
       "      <th>ic_others_9</th>\n",
       "      <th>total_rech_num_6</th>\n",
       "      <th>total_rech_num_7</th>\n",
       "      <th>total_rech_num_8</th>\n",
       "      <th>total_rech_num_9</th>\n",
       "      <th>total_rech_amt_6</th>\n",
       "      <th>total_rech_amt_7</th>\n",
       "      <th>total_rech_amt_8</th>\n",
       "      <th>total_rech_amt_9</th>\n",
       "      <th>max_rech_amt_6</th>\n",
       "      <th>max_rech_amt_7</th>\n",
       "      <th>max_rech_amt_8</th>\n",
       "      <th>max_rech_amt_9</th>\n",
       "      <th>date_of_last_rech_6</th>\n",
       "      <th>date_of_last_rech_7</th>\n",
       "      <th>date_of_last_rech_8</th>\n",
       "      <th>date_of_last_rech_9</th>\n",
       "      <th>last_day_rch_amt_6</th>\n",
       "      <th>last_day_rch_amt_7</th>\n",
       "      <th>last_day_rch_amt_8</th>\n",
       "      <th>last_day_rch_amt_9</th>\n",
       "      <th>date_of_last_rech_data_6</th>\n",
       "      <th>date_of_last_rech_data_7</th>\n",
       "      <th>date_of_last_rech_data_8</th>\n",
       "      <th>date_of_last_rech_data_9</th>\n",
       "      <th>total_rech_data_6</th>\n",
       "      <th>total_rech_data_7</th>\n",
       "      <th>total_rech_data_8</th>\n",
       "      <th>total_rech_data_9</th>\n",
       "      <th>max_rech_data_6</th>\n",
       "      <th>max_rech_data_7</th>\n",
       "      <th>max_rech_data_8</th>\n",
       "      <th>max_rech_data_9</th>\n",
       "      <th>count_rech_2g_6</th>\n",
       "      <th>count_rech_2g_7</th>\n",
       "      <th>count_rech_2g_8</th>\n",
       "      <th>count_rech_2g_9</th>\n",
       "      <th>count_rech_3g_6</th>\n",
       "      <th>count_rech_3g_7</th>\n",
       "      <th>count_rech_3g_8</th>\n",
       "      <th>count_rech_3g_9</th>\n",
       "      <th>av_rech_amt_data_6</th>\n",
       "      <th>av_rech_amt_data_7</th>\n",
       "      <th>av_rech_amt_data_8</th>\n",
       "      <th>av_rech_amt_data_9</th>\n",
       "      <th>vol_2g_mb_6</th>\n",
       "      <th>vol_2g_mb_7</th>\n",
       "      <th>vol_2g_mb_8</th>\n",
       "      <th>vol_2g_mb_9</th>\n",
       "      <th>vol_3g_mb_6</th>\n",
       "      <th>vol_3g_mb_7</th>\n",
       "      <th>vol_3g_mb_8</th>\n",
       "      <th>vol_3g_mb_9</th>\n",
       "      <th>arpu_3g_6</th>\n",
       "      <th>arpu_3g_7</th>\n",
       "      <th>arpu_3g_8</th>\n",
       "      <th>arpu_3g_9</th>\n",
       "      <th>arpu_2g_6</th>\n",
       "      <th>arpu_2g_7</th>\n",
       "      <th>arpu_2g_8</th>\n",
       "      <th>arpu_2g_9</th>\n",
       "      <th>night_pck_user_6</th>\n",
       "      <th>night_pck_user_7</th>\n",
       "      <th>night_pck_user_8</th>\n",
       "      <th>night_pck_user_9</th>\n",
       "      <th>monthly_2g_6</th>\n",
       "      <th>monthly_2g_7</th>\n",
       "      <th>monthly_2g_8</th>\n",
       "      <th>monthly_2g_9</th>\n",
       "      <th>sachet_2g_6</th>\n",
       "      <th>sachet_2g_7</th>\n",
       "      <th>sachet_2g_8</th>\n",
       "      <th>sachet_2g_9</th>\n",
       "      <th>monthly_3g_6</th>\n",
       "      <th>monthly_3g_7</th>\n",
       "      <th>monthly_3g_8</th>\n",
       "      <th>monthly_3g_9</th>\n",
       "      <th>sachet_3g_6</th>\n",
       "      <th>sachet_3g_7</th>\n",
       "      <th>sachet_3g_8</th>\n",
       "      <th>sachet_3g_9</th>\n",
       "      <th>fb_user_6</th>\n",
       "      <th>fb_user_7</th>\n",
       "      <th>fb_user_8</th>\n",
       "      <th>fb_user_9</th>\n",
       "      <th>aon</th>\n",
       "      <th>aug_vbc_3g</th>\n",
       "      <th>jul_vbc_3g</th>\n",
       "      <th>jun_vbc_3g</th>\n",
       "      <th>sep_vbc_3g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7000842753</td>\n",
       "      <td>109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6/30/2014</td>\n",
       "      <td>7/31/2014</td>\n",
       "      <td>8/31/2014</td>\n",
       "      <td>9/30/2014</td>\n",
       "      <td>197.385</td>\n",
       "      <td>214.816</td>\n",
       "      <td>213.803</td>\n",
       "      <td>21.100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>362</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>0</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>0</td>\n",
       "      <td>6/21/2014</td>\n",
       "      <td>7/16/2014</td>\n",
       "      <td>8/8/2014</td>\n",
       "      <td>9/28/2014</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>0</td>\n",
       "      <td>6/21/2014</td>\n",
       "      <td>7/16/2014</td>\n",
       "      <td>8/8/2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>252.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>252.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.13</td>\n",
       "      <td>1.32</td>\n",
       "      <td>5.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.57</td>\n",
       "      <td>150.76</td>\n",
       "      <td>109.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>212.17</td>\n",
       "      <td>212.17</td>\n",
       "      <td>212.17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>212.17</td>\n",
       "      <td>212.17</td>\n",
       "      <td>212.17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>968</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.20</td>\n",
       "      <td>3.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7001865778</td>\n",
       "      <td>109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6/30/2014</td>\n",
       "      <td>7/31/2014</td>\n",
       "      <td>8/31/2014</td>\n",
       "      <td>9/30/2014</td>\n",
       "      <td>34.047</td>\n",
       "      <td>355.074</td>\n",
       "      <td>268.321</td>\n",
       "      <td>86.285</td>\n",
       "      <td>24.11</td>\n",
       "      <td>78.68</td>\n",
       "      <td>7.68</td>\n",
       "      <td>18.34</td>\n",
       "      <td>15.74</td>\n",
       "      <td>99.84</td>\n",
       "      <td>304.76</td>\n",
       "      <td>53.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23.88</td>\n",
       "      <td>74.56</td>\n",
       "      <td>7.68</td>\n",
       "      <td>18.34</td>\n",
       "      <td>11.51</td>\n",
       "      <td>75.94</td>\n",
       "      <td>291.86</td>\n",
       "      <td>53.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>35.39</td>\n",
       "      <td>150.51</td>\n",
       "      <td>299.54</td>\n",
       "      <td>72.11</td>\n",
       "      <td>0.23</td>\n",
       "      <td>4.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.68</td>\n",
       "      <td>23.43</td>\n",
       "      <td>12.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.31</td>\n",
       "      <td>178.53</td>\n",
       "      <td>312.44</td>\n",
       "      <td>72.11</td>\n",
       "      <td>1.61</td>\n",
       "      <td>29.91</td>\n",
       "      <td>29.23</td>\n",
       "      <td>116.09</td>\n",
       "      <td>17.48</td>\n",
       "      <td>65.38</td>\n",
       "      <td>375.58</td>\n",
       "      <td>56.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.93</td>\n",
       "      <td>3.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.09</td>\n",
       "      <td>104.23</td>\n",
       "      <td>408.43</td>\n",
       "      <td>173.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.49</td>\n",
       "      <td>15.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.84</td>\n",
       "      <td>15.01</td>\n",
       "      <td>26.83</td>\n",
       "      <td>104.23</td>\n",
       "      <td>423.28</td>\n",
       "      <td>188.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>74</td>\n",
       "      <td>384</td>\n",
       "      <td>283</td>\n",
       "      <td>121</td>\n",
       "      <td>44</td>\n",
       "      <td>154</td>\n",
       "      <td>65</td>\n",
       "      <td>50</td>\n",
       "      <td>6/29/2014</td>\n",
       "      <td>7/31/2014</td>\n",
       "      <td>8/28/2014</td>\n",
       "      <td>9/30/2014</td>\n",
       "      <td>44</td>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7/25/2014</td>\n",
       "      <td>8/10/2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>108.07</td>\n",
       "      <td>365.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.61</td>\n",
       "      <td>7.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7001625959</td>\n",
       "      <td>109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6/30/2014</td>\n",
       "      <td>7/31/2014</td>\n",
       "      <td>8/31/2014</td>\n",
       "      <td>9/30/2014</td>\n",
       "      <td>167.690</td>\n",
       "      <td>189.058</td>\n",
       "      <td>210.226</td>\n",
       "      <td>290.714</td>\n",
       "      <td>11.54</td>\n",
       "      <td>55.24</td>\n",
       "      <td>37.26</td>\n",
       "      <td>74.81</td>\n",
       "      <td>143.33</td>\n",
       "      <td>220.59</td>\n",
       "      <td>208.36</td>\n",
       "      <td>118.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>38.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>70.94</td>\n",
       "      <td>7.19</td>\n",
       "      <td>28.74</td>\n",
       "      <td>13.58</td>\n",
       "      <td>14.39</td>\n",
       "      <td>29.34</td>\n",
       "      <td>16.86</td>\n",
       "      <td>38.46</td>\n",
       "      <td>28.16</td>\n",
       "      <td>24.11</td>\n",
       "      <td>21.79</td>\n",
       "      <td>15.61</td>\n",
       "      <td>22.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>135.54</td>\n",
       "      <td>45.76</td>\n",
       "      <td>0.48</td>\n",
       "      <td>60.66</td>\n",
       "      <td>67.41</td>\n",
       "      <td>67.66</td>\n",
       "      <td>64.81</td>\n",
       "      <td>4.34</td>\n",
       "      <td>26.49</td>\n",
       "      <td>22.58</td>\n",
       "      <td>8.76</td>\n",
       "      <td>41.81</td>\n",
       "      <td>67.41</td>\n",
       "      <td>75.53</td>\n",
       "      <td>9.28</td>\n",
       "      <td>1.48</td>\n",
       "      <td>14.76</td>\n",
       "      <td>22.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.64</td>\n",
       "      <td>108.68</td>\n",
       "      <td>120.94</td>\n",
       "      <td>18.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.56</td>\n",
       "      <td>236.84</td>\n",
       "      <td>96.84</td>\n",
       "      <td>42.08</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>155.33</td>\n",
       "      <td>412.94</td>\n",
       "      <td>285.46</td>\n",
       "      <td>124.94</td>\n",
       "      <td>115.69</td>\n",
       "      <td>71.11</td>\n",
       "      <td>67.46</td>\n",
       "      <td>148.23</td>\n",
       "      <td>14.38</td>\n",
       "      <td>15.44</td>\n",
       "      <td>38.89</td>\n",
       "      <td>38.98</td>\n",
       "      <td>99.48</td>\n",
       "      <td>122.29</td>\n",
       "      <td>49.63</td>\n",
       "      <td>158.19</td>\n",
       "      <td>229.56</td>\n",
       "      <td>208.86</td>\n",
       "      <td>155.99</td>\n",
       "      <td>345.41</td>\n",
       "      <td>72.41</td>\n",
       "      <td>71.29</td>\n",
       "      <td>28.69</td>\n",
       "      <td>49.44</td>\n",
       "      <td>45.18</td>\n",
       "      <td>177.01</td>\n",
       "      <td>167.09</td>\n",
       "      <td>118.18</td>\n",
       "      <td>21.73</td>\n",
       "      <td>58.34</td>\n",
       "      <td>43.23</td>\n",
       "      <td>3.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>139.33</td>\n",
       "      <td>306.66</td>\n",
       "      <td>239.03</td>\n",
       "      <td>171.49</td>\n",
       "      <td>370.04</td>\n",
       "      <td>519.53</td>\n",
       "      <td>395.03</td>\n",
       "      <td>517.74</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.36</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>168</td>\n",
       "      <td>315</td>\n",
       "      <td>116</td>\n",
       "      <td>358</td>\n",
       "      <td>86</td>\n",
       "      <td>200</td>\n",
       "      <td>86</td>\n",
       "      <td>100</td>\n",
       "      <td>6/17/2014</td>\n",
       "      <td>7/24/2014</td>\n",
       "      <td>8/14/2014</td>\n",
       "      <td>9/29/2014</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9/17/2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1103</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.17</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7001204172</td>\n",
       "      <td>109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6/30/2014</td>\n",
       "      <td>7/31/2014</td>\n",
       "      <td>8/31/2014</td>\n",
       "      <td>9/30/2014</td>\n",
       "      <td>221.338</td>\n",
       "      <td>251.102</td>\n",
       "      <td>508.054</td>\n",
       "      <td>389.500</td>\n",
       "      <td>99.91</td>\n",
       "      <td>54.39</td>\n",
       "      <td>310.98</td>\n",
       "      <td>241.71</td>\n",
       "      <td>123.31</td>\n",
       "      <td>109.01</td>\n",
       "      <td>71.68</td>\n",
       "      <td>113.54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.86</td>\n",
       "      <td>44.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.09</td>\n",
       "      <td>39.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>73.68</td>\n",
       "      <td>34.81</td>\n",
       "      <td>10.61</td>\n",
       "      <td>15.49</td>\n",
       "      <td>107.43</td>\n",
       "      <td>83.21</td>\n",
       "      <td>22.46</td>\n",
       "      <td>65.46</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.65</td>\n",
       "      <td>4.91</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>183.03</td>\n",
       "      <td>118.68</td>\n",
       "      <td>37.99</td>\n",
       "      <td>83.03</td>\n",
       "      <td>26.23</td>\n",
       "      <td>14.89</td>\n",
       "      <td>289.58</td>\n",
       "      <td>226.21</td>\n",
       "      <td>2.99</td>\n",
       "      <td>1.73</td>\n",
       "      <td>6.53</td>\n",
       "      <td>9.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.23</td>\n",
       "      <td>16.63</td>\n",
       "      <td>296.11</td>\n",
       "      <td>236.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.09</td>\n",
       "      <td>43.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>223.23</td>\n",
       "      <td>135.31</td>\n",
       "      <td>352.21</td>\n",
       "      <td>362.54</td>\n",
       "      <td>62.08</td>\n",
       "      <td>19.98</td>\n",
       "      <td>8.04</td>\n",
       "      <td>41.73</td>\n",
       "      <td>113.96</td>\n",
       "      <td>64.51</td>\n",
       "      <td>20.28</td>\n",
       "      <td>52.86</td>\n",
       "      <td>57.43</td>\n",
       "      <td>27.09</td>\n",
       "      <td>19.84</td>\n",
       "      <td>65.59</td>\n",
       "      <td>233.48</td>\n",
       "      <td>111.59</td>\n",
       "      <td>48.18</td>\n",
       "      <td>160.19</td>\n",
       "      <td>43.48</td>\n",
       "      <td>66.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>129.84</td>\n",
       "      <td>1.33</td>\n",
       "      <td>38.56</td>\n",
       "      <td>4.94</td>\n",
       "      <td>13.98</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.99</td>\n",
       "      <td>105.01</td>\n",
       "      <td>4.94</td>\n",
       "      <td>143.83</td>\n",
       "      <td>280.08</td>\n",
       "      <td>216.61</td>\n",
       "      <td>53.13</td>\n",
       "      <td>305.38</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>230</td>\n",
       "      <td>310</td>\n",
       "      <td>601</td>\n",
       "      <td>410</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>6/28/2014</td>\n",
       "      <td>7/31/2014</td>\n",
       "      <td>8/31/2014</td>\n",
       "      <td>9/30/2014</td>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2491</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7000142493</td>\n",
       "      <td>109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6/30/2014</td>\n",
       "      <td>7/31/2014</td>\n",
       "      <td>8/31/2014</td>\n",
       "      <td>9/30/2014</td>\n",
       "      <td>261.636</td>\n",
       "      <td>309.876</td>\n",
       "      <td>238.174</td>\n",
       "      <td>163.426</td>\n",
       "      <td>50.31</td>\n",
       "      <td>149.44</td>\n",
       "      <td>83.89</td>\n",
       "      <td>58.78</td>\n",
       "      <td>76.96</td>\n",
       "      <td>91.88</td>\n",
       "      <td>124.26</td>\n",
       "      <td>45.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>50.31</td>\n",
       "      <td>149.44</td>\n",
       "      <td>83.89</td>\n",
       "      <td>58.78</td>\n",
       "      <td>67.64</td>\n",
       "      <td>91.88</td>\n",
       "      <td>124.26</td>\n",
       "      <td>37.89</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>117.96</td>\n",
       "      <td>241.33</td>\n",
       "      <td>208.16</td>\n",
       "      <td>98.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.98</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>127.28</td>\n",
       "      <td>241.33</td>\n",
       "      <td>208.16</td>\n",
       "      <td>104.59</td>\n",
       "      <td>105.68</td>\n",
       "      <td>88.49</td>\n",
       "      <td>233.81</td>\n",
       "      <td>154.56</td>\n",
       "      <td>106.84</td>\n",
       "      <td>109.54</td>\n",
       "      <td>104.13</td>\n",
       "      <td>48.24</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>214.03</td>\n",
       "      <td>198.04</td>\n",
       "      <td>337.94</td>\n",
       "      <td>202.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.86</td>\n",
       "      <td>2.31</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.86</td>\n",
       "      <td>2.31</td>\n",
       "      <td>216.44</td>\n",
       "      <td>198.29</td>\n",
       "      <td>338.81</td>\n",
       "      <td>205.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>196</td>\n",
       "      <td>350</td>\n",
       "      <td>287</td>\n",
       "      <td>200</td>\n",
       "      <td>56</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>50</td>\n",
       "      <td>6/26/2014</td>\n",
       "      <td>7/28/2014</td>\n",
       "      <td>8/9/2014</td>\n",
       "      <td>9/28/2014</td>\n",
       "      <td>50</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>50</td>\n",
       "      <td>6/4/2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mobile_number  circle_id  loc_og_t2o_mou  std_og_t2o_mou  loc_ic_t2o_mou  \\\n",
       "0     7000842753        109             0.0             0.0             0.0   \n",
       "1     7001865778        109             0.0             0.0             0.0   \n",
       "2     7001625959        109             0.0             0.0             0.0   \n",
       "3     7001204172        109             0.0             0.0             0.0   \n",
       "4     7000142493        109             0.0             0.0             0.0   \n",
       "\n",
       "  last_date_of_month_6 last_date_of_month_7 last_date_of_month_8  \\\n",
       "0            6/30/2014            7/31/2014            8/31/2014   \n",
       "1            6/30/2014            7/31/2014            8/31/2014   \n",
       "2            6/30/2014            7/31/2014            8/31/2014   \n",
       "3            6/30/2014            7/31/2014            8/31/2014   \n",
       "4            6/30/2014            7/31/2014            8/31/2014   \n",
       "\n",
       "  last_date_of_month_9   arpu_6   arpu_7   arpu_8   arpu_9  onnet_mou_6  \\\n",
       "0            9/30/2014  197.385  214.816  213.803   21.100          NaN   \n",
       "1            9/30/2014   34.047  355.074  268.321   86.285        24.11   \n",
       "2            9/30/2014  167.690  189.058  210.226  290.714        11.54   \n",
       "3            9/30/2014  221.338  251.102  508.054  389.500        99.91   \n",
       "4            9/30/2014  261.636  309.876  238.174  163.426        50.31   \n",
       "\n",
       "   onnet_mou_7  onnet_mou_8  onnet_mou_9  offnet_mou_6  offnet_mou_7  \\\n",
       "0          NaN         0.00          NaN           NaN           NaN   \n",
       "1        78.68         7.68        18.34         15.74         99.84   \n",
       "2        55.24        37.26        74.81        143.33        220.59   \n",
       "3        54.39       310.98       241.71        123.31        109.01   \n",
       "4       149.44        83.89        58.78         76.96         91.88   \n",
       "\n",
       "   offnet_mou_8  offnet_mou_9  roam_ic_mou_6  roam_ic_mou_7  roam_ic_mou_8  \\\n",
       "0          0.00           NaN            NaN            NaN           0.00   \n",
       "1        304.76         53.76            0.0           0.00           0.00   \n",
       "2        208.36        118.91            0.0           0.00           0.00   \n",
       "3         71.68        113.54            0.0          54.86          44.38   \n",
       "4        124.26         45.81            0.0           0.00           0.00   \n",
       "\n",
       "   roam_ic_mou_9  roam_og_mou_6  roam_og_mou_7  roam_og_mou_8  roam_og_mou_9  \\\n",
       "0            NaN            NaN            NaN           0.00            NaN   \n",
       "1           0.00            0.0           0.00           0.00           0.00   \n",
       "2          38.49            0.0           0.00           0.00          70.94   \n",
       "3           0.00            0.0          28.09          39.04           0.00   \n",
       "4           0.00            0.0           0.00           0.00           0.00   \n",
       "\n",
       "   loc_og_t2t_mou_6  loc_og_t2t_mou_7  loc_og_t2t_mou_8  loc_og_t2t_mou_9  \\\n",
       "0               NaN               NaN              0.00               NaN   \n",
       "1             23.88             74.56              7.68             18.34   \n",
       "2              7.19             28.74             13.58             14.39   \n",
       "3             73.68             34.81             10.61             15.49   \n",
       "4             50.31            149.44             83.89             58.78   \n",
       "\n",
       "   loc_og_t2m_mou_6  loc_og_t2m_mou_7  loc_og_t2m_mou_8  loc_og_t2m_mou_9  \\\n",
       "0               NaN               NaN              0.00               NaN   \n",
       "1             11.51             75.94            291.86             53.76   \n",
       "2             29.34             16.86             38.46             28.16   \n",
       "3            107.43             83.21             22.46             65.46   \n",
       "4             67.64             91.88            124.26             37.89   \n",
       "\n",
       "   loc_og_t2f_mou_6  loc_og_t2f_mou_7  loc_og_t2f_mou_8  loc_og_t2f_mou_9  \\\n",
       "0               NaN               NaN              0.00               NaN   \n",
       "1              0.00              0.00              0.00              0.00   \n",
       "2             24.11             21.79             15.61             22.24   \n",
       "3              1.91              0.65              4.91              2.06   \n",
       "4              0.00              0.00              0.00              1.93   \n",
       "\n",
       "   loc_og_t2c_mou_6  loc_og_t2c_mou_7  loc_og_t2c_mou_8  loc_og_t2c_mou_9  \\\n",
       "0               NaN               NaN              0.00               NaN   \n",
       "1               0.0              2.91              0.00              0.00   \n",
       "2               0.0            135.54             45.76              0.48   \n",
       "3               0.0              0.00              0.00              0.00   \n",
       "4               0.0              0.00              0.00              0.00   \n",
       "\n",
       "   loc_og_mou_6  loc_og_mou_7  loc_og_mou_8  loc_og_mou_9  std_og_t2t_mou_6  \\\n",
       "0           NaN           NaN          0.00           NaN               NaN   \n",
       "1         35.39        150.51        299.54         72.11              0.23   \n",
       "2         60.66         67.41         67.66         64.81              4.34   \n",
       "3        183.03        118.68         37.99         83.03             26.23   \n",
       "4        117.96        241.33        208.16         98.61              0.00   \n",
       "\n",
       "   std_og_t2t_mou_7  std_og_t2t_mou_8  std_og_t2t_mou_9  std_og_t2m_mou_6  \\\n",
       "0               NaN              0.00               NaN               NaN   \n",
       "1              4.11              0.00              0.00              0.00   \n",
       "2             26.49             22.58              8.76             41.81   \n",
       "3             14.89            289.58            226.21              2.99   \n",
       "4              0.00              0.00              0.00              9.31   \n",
       "\n",
       "   std_og_t2m_mou_7  std_og_t2m_mou_8  std_og_t2m_mou_9  std_og_t2f_mou_6  \\\n",
       "0               NaN              0.00               NaN               NaN   \n",
       "1              0.46              0.13              0.00              0.00   \n",
       "2             67.41             75.53              9.28              1.48   \n",
       "3              1.73              6.53              9.99              0.00   \n",
       "4              0.00              0.00              0.00              0.00   \n",
       "\n",
       "   std_og_t2f_mou_7  std_og_t2f_mou_8  std_og_t2f_mou_9  std_og_t2c_mou_6  \\\n",
       "0               NaN              0.00               NaN               NaN   \n",
       "1              0.00              0.00               0.0               0.0   \n",
       "2             14.76             22.83               0.0               0.0   \n",
       "3              0.00              0.00               0.0               0.0   \n",
       "4              0.00              0.00               0.0               0.0   \n",
       "\n",
       "   std_og_t2c_mou_7  std_og_t2c_mou_8  std_og_t2c_mou_9  std_og_mou_6  \\\n",
       "0               NaN               0.0               NaN           NaN   \n",
       "1               0.0               0.0               0.0          0.23   \n",
       "2               0.0               0.0               0.0         47.64   \n",
       "3               0.0               0.0               0.0         29.23   \n",
       "4               0.0               0.0               0.0          9.31   \n",
       "\n",
       "   std_og_mou_7  std_og_mou_8  std_og_mou_9  isd_og_mou_6  isd_og_mou_7  \\\n",
       "0           NaN          0.00           NaN           NaN           NaN   \n",
       "1          4.58          0.13          0.00           0.0           0.0   \n",
       "2        108.68        120.94         18.04           0.0           0.0   \n",
       "3         16.63        296.11        236.21           0.0           0.0   \n",
       "4          0.00          0.00          0.00           0.0           0.0   \n",
       "\n",
       "   isd_og_mou_8  isd_og_mou_9  spl_og_mou_6  spl_og_mou_7  spl_og_mou_8  \\\n",
       "0           0.0           NaN           NaN           NaN          0.00   \n",
       "1           0.0           0.0          4.68         23.43         12.76   \n",
       "2           0.0           0.0         46.56        236.84         96.84   \n",
       "3           0.0           0.0         10.96          0.00         18.09   \n",
       "4           0.0           0.0          0.00          0.00          0.00   \n",
       "\n",
       "   spl_og_mou_9  og_others_6  og_others_7  og_others_8  og_others_9  \\\n",
       "0           NaN          NaN          NaN          0.0          NaN   \n",
       "1          0.00         0.00          0.0          0.0          0.0   \n",
       "2         42.08         0.45          0.0          0.0          0.0   \n",
       "3         43.29         0.00          0.0          0.0          0.0   \n",
       "4          5.98         0.00          0.0          0.0          0.0   \n",
       "\n",
       "   total_og_mou_6  total_og_mou_7  total_og_mou_8  total_og_mou_9  \\\n",
       "0            0.00            0.00            0.00            0.00   \n",
       "1           40.31          178.53          312.44           72.11   \n",
       "2          155.33          412.94          285.46          124.94   \n",
       "3          223.23          135.31          352.21          362.54   \n",
       "4          127.28          241.33          208.16          104.59   \n",
       "\n",
       "   loc_ic_t2t_mou_6  loc_ic_t2t_mou_7  loc_ic_t2t_mou_8  loc_ic_t2t_mou_9  \\\n",
       "0               NaN               NaN              0.16               NaN   \n",
       "1              1.61             29.91             29.23            116.09   \n",
       "2            115.69             71.11             67.46            148.23   \n",
       "3             62.08             19.98              8.04             41.73   \n",
       "4            105.68             88.49            233.81            154.56   \n",
       "\n",
       "   loc_ic_t2m_mou_6  loc_ic_t2m_mou_7  loc_ic_t2m_mou_8  loc_ic_t2m_mou_9  \\\n",
       "0               NaN               NaN              4.13               NaN   \n",
       "1             17.48             65.38            375.58             56.93   \n",
       "2             14.38             15.44             38.89             38.98   \n",
       "3            113.96             64.51             20.28             52.86   \n",
       "4            106.84            109.54            104.13             48.24   \n",
       "\n",
       "   loc_ic_t2f_mou_6  loc_ic_t2f_mou_7  loc_ic_t2f_mou_8  loc_ic_t2f_mou_9  \\\n",
       "0               NaN               NaN              1.15               NaN   \n",
       "1              0.00              8.93              3.61              0.00   \n",
       "2             99.48            122.29             49.63            158.19   \n",
       "3             57.43             27.09             19.84             65.59   \n",
       "4              1.50              0.00              0.00              0.00   \n",
       "\n",
       "   loc_ic_mou_6  loc_ic_mou_7  loc_ic_mou_8  loc_ic_mou_9  std_ic_t2t_mou_6  \\\n",
       "0           NaN           NaN          5.44           NaN               NaN   \n",
       "1         19.09        104.23        408.43        173.03              0.00   \n",
       "2        229.56        208.86        155.99        345.41             72.41   \n",
       "3        233.48        111.59         48.18        160.19             43.48   \n",
       "4        214.03        198.04        337.94        202.81              0.00   \n",
       "\n",
       "   std_ic_t2t_mou_7  std_ic_t2t_mou_8  std_ic_t2t_mou_9  std_ic_t2m_mou_6  \\\n",
       "0               NaN              0.00               NaN               NaN   \n",
       "1              0.00              2.35              0.00              5.90   \n",
       "2             71.29             28.69             49.44             45.18   \n",
       "3             66.44              0.00            129.84              1.33   \n",
       "4              0.00              0.86              2.31              1.93   \n",
       "\n",
       "   std_ic_t2m_mou_7  std_ic_t2m_mou_8  std_ic_t2m_mou_9  std_ic_t2f_mou_6  \\\n",
       "0               NaN              0.00               NaN               NaN   \n",
       "1              0.00             12.49             15.01              0.00   \n",
       "2            177.01            167.09            118.18             21.73   \n",
       "3             38.56              4.94             13.98              1.18   \n",
       "4              0.25              0.00              0.00              0.00   \n",
       "\n",
       "   std_ic_t2f_mou_7  std_ic_t2f_mou_8  std_ic_t2f_mou_9  std_ic_t2o_mou_6  \\\n",
       "0               NaN              0.00               NaN               NaN   \n",
       "1              0.00              0.00              0.00               0.0   \n",
       "2             58.34             43.23              3.86               0.0   \n",
       "3              0.00              0.00              0.00               0.0   \n",
       "4              0.00              0.00              0.00               0.0   \n",
       "\n",
       "   std_ic_t2o_mou_7  std_ic_t2o_mou_8  std_ic_t2o_mou_9  std_ic_mou_6  \\\n",
       "0               NaN               0.0               NaN           NaN   \n",
       "1               0.0               0.0               0.0          5.90   \n",
       "2               0.0               0.0               0.0        139.33   \n",
       "3               0.0               0.0               0.0         45.99   \n",
       "4               0.0               0.0               0.0          1.93   \n",
       "\n",
       "   std_ic_mou_7  std_ic_mou_8  std_ic_mou_9  total_ic_mou_6  total_ic_mou_7  \\\n",
       "0           NaN          0.00           NaN            0.00            0.00   \n",
       "1          0.00         14.84         15.01           26.83          104.23   \n",
       "2        306.66        239.03        171.49          370.04          519.53   \n",
       "3        105.01          4.94        143.83          280.08          216.61   \n",
       "4          0.25          0.86          2.31          216.44          198.29   \n",
       "\n",
       "   total_ic_mou_8  total_ic_mou_9  spl_ic_mou_6  spl_ic_mou_7  spl_ic_mou_8  \\\n",
       "0            5.44            0.00           NaN           NaN           0.0   \n",
       "1          423.28          188.04          0.00           0.0           0.0   \n",
       "2          395.03          517.74          0.21           0.0           0.0   \n",
       "3           53.13          305.38          0.59           0.0           0.0   \n",
       "4          338.81          205.31          0.00           0.0           0.0   \n",
       "\n",
       "   spl_ic_mou_9  isd_ic_mou_6  isd_ic_mou_7  isd_ic_mou_8  isd_ic_mou_9  \\\n",
       "0           NaN           NaN           NaN           0.0           NaN   \n",
       "1          0.00          1.83          0.00           0.0          0.00   \n",
       "2          0.45          0.00          0.85           0.0          0.01   \n",
       "3          0.55          0.00          0.00           0.0          0.00   \n",
       "4          0.18          0.00          0.00           0.0          0.00   \n",
       "\n",
       "   ic_others_6  ic_others_7  ic_others_8  ic_others_9  total_rech_num_6  \\\n",
       "0          NaN          NaN          0.0          NaN                 4   \n",
       "1         0.00         0.00          0.0         0.00                 4   \n",
       "2         0.93         3.14          0.0         0.36                 5   \n",
       "3         0.00         0.00          0.0         0.80                10   \n",
       "4         0.48         0.00          0.0         0.00                 5   \n",
       "\n",
       "   total_rech_num_7  total_rech_num_8  total_rech_num_9  total_rech_amt_6  \\\n",
       "0                 3                 2                 6               362   \n",
       "1                 9                11                 5                74   \n",
       "2                 4                 2                 7               168   \n",
       "3                11                18                14               230   \n",
       "4                 6                 3                 4               196   \n",
       "\n",
       "   total_rech_amt_7  total_rech_amt_8  total_rech_amt_9  max_rech_amt_6  \\\n",
       "0               252               252                 0             252   \n",
       "1               384               283               121              44   \n",
       "2               315               116               358              86   \n",
       "3               310               601               410              60   \n",
       "4               350               287               200              56   \n",
       "\n",
       "   max_rech_amt_7  max_rech_amt_8  max_rech_amt_9 date_of_last_rech_6  \\\n",
       "0             252             252               0           6/21/2014   \n",
       "1             154              65              50           6/29/2014   \n",
       "2             200              86             100           6/17/2014   \n",
       "3              50              50              50           6/28/2014   \n",
       "4             110             110              50           6/26/2014   \n",
       "\n",
       "  date_of_last_rech_7 date_of_last_rech_8 date_of_last_rech_9  \\\n",
       "0           7/16/2014            8/8/2014           9/28/2014   \n",
       "1           7/31/2014           8/28/2014           9/30/2014   \n",
       "2           7/24/2014           8/14/2014           9/29/2014   \n",
       "3           7/31/2014           8/31/2014           9/30/2014   \n",
       "4           7/28/2014            8/9/2014           9/28/2014   \n",
       "\n",
       "   last_day_rch_amt_6  last_day_rch_amt_7  last_day_rch_amt_8  \\\n",
       "0                 252                 252                 252   \n",
       "1                  44                  23                  30   \n",
       "2                   0                 200                  86   \n",
       "3                  30                  50                  50   \n",
       "4                  50                 110                 110   \n",
       "\n",
       "   last_day_rch_amt_9 date_of_last_rech_data_6 date_of_last_rech_data_7  \\\n",
       "0                   0                6/21/2014                7/16/2014   \n",
       "1                   0                      NaN                7/25/2014   \n",
       "2                   0                      NaN                      NaN   \n",
       "3                  30                      NaN                      NaN   \n",
       "4                  50                 6/4/2014                      NaN   \n",
       "\n",
       "  date_of_last_rech_data_8 date_of_last_rech_data_9  total_rech_data_6  \\\n",
       "0                 8/8/2014                      NaN                1.0   \n",
       "1                8/10/2014                      NaN                NaN   \n",
       "2                      NaN                9/17/2014                NaN   \n",
       "3                      NaN                      NaN                NaN   \n",
       "4                      NaN                      NaN                1.0   \n",
       "\n",
       "   total_rech_data_7  total_rech_data_8  total_rech_data_9  max_rech_data_6  \\\n",
       "0                1.0                1.0                NaN            252.0   \n",
       "1                1.0                2.0                NaN              NaN   \n",
       "2                NaN                NaN                1.0              NaN   \n",
       "3                NaN                NaN                NaN              NaN   \n",
       "4                NaN                NaN                NaN             56.0   \n",
       "\n",
       "   max_rech_data_7  max_rech_data_8  max_rech_data_9  count_rech_2g_6  \\\n",
       "0            252.0            252.0              NaN              0.0   \n",
       "1            154.0             25.0              NaN              NaN   \n",
       "2              NaN              NaN             46.0              NaN   \n",
       "3              NaN              NaN              NaN              NaN   \n",
       "4              NaN              NaN              NaN              1.0   \n",
       "\n",
       "   count_rech_2g_7  count_rech_2g_8  count_rech_2g_9  count_rech_3g_6  \\\n",
       "0              0.0              0.0              NaN              1.0   \n",
       "1              1.0              2.0              NaN              NaN   \n",
       "2              NaN              NaN              1.0              NaN   \n",
       "3              NaN              NaN              NaN              NaN   \n",
       "4              NaN              NaN              NaN              0.0   \n",
       "\n",
       "   count_rech_3g_7  count_rech_3g_8  count_rech_3g_9  av_rech_amt_data_6  \\\n",
       "0              1.0              1.0              NaN               252.0   \n",
       "1              0.0              0.0              NaN                 NaN   \n",
       "2              NaN              NaN              0.0                 NaN   \n",
       "3              NaN              NaN              NaN                 NaN   \n",
       "4              NaN              NaN              NaN                56.0   \n",
       "\n",
       "   av_rech_amt_data_7  av_rech_amt_data_8  av_rech_amt_data_9  vol_2g_mb_6  \\\n",
       "0               252.0               252.0                 NaN        30.13   \n",
       "1               154.0                50.0                 NaN         0.00   \n",
       "2                 NaN                 NaN                46.0         0.00   \n",
       "3                 NaN                 NaN                 NaN         0.00   \n",
       "4                 NaN                 NaN                 NaN         0.00   \n",
       "\n",
       "   vol_2g_mb_7  vol_2g_mb_8  vol_2g_mb_9  vol_3g_mb_6  vol_3g_mb_7  \\\n",
       "0         1.32         5.75          0.0        83.57       150.76   \n",
       "1       108.07       365.47          0.0         0.00         0.00   \n",
       "2         0.00         0.00          0.0         0.00         0.00   \n",
       "3         0.00         0.00          0.0         0.00         0.00   \n",
       "4         0.00         0.00          0.0         0.00         0.00   \n",
       "\n",
       "   vol_3g_mb_8  vol_3g_mb_9  arpu_3g_6  arpu_3g_7  arpu_3g_8  arpu_3g_9  \\\n",
       "0       109.61         0.00     212.17     212.17     212.17        NaN   \n",
       "1         0.00         0.00        NaN       0.00       0.00        NaN   \n",
       "2         0.00         8.42        NaN        NaN        NaN       2.84   \n",
       "3         0.00         0.00        NaN        NaN        NaN        NaN   \n",
       "4         0.00         0.00       0.00        NaN        NaN        NaN   \n",
       "\n",
       "   arpu_2g_6  arpu_2g_7  arpu_2g_8  arpu_2g_9  night_pck_user_6  \\\n",
       "0     212.17     212.17     212.17        NaN               0.0   \n",
       "1        NaN      28.61       7.60        NaN               NaN   \n",
       "2        NaN        NaN        NaN        0.0               NaN   \n",
       "3        NaN        NaN        NaN        NaN               NaN   \n",
       "4       0.00        NaN        NaN        NaN               0.0   \n",
       "\n",
       "   night_pck_user_7  night_pck_user_8  night_pck_user_9  monthly_2g_6  \\\n",
       "0               0.0               0.0               NaN             0   \n",
       "1               0.0               0.0               NaN             0   \n",
       "2               NaN               NaN               0.0             0   \n",
       "3               NaN               NaN               NaN             0   \n",
       "4               NaN               NaN               NaN             0   \n",
       "\n",
       "   monthly_2g_7  monthly_2g_8  monthly_2g_9  sachet_2g_6  sachet_2g_7  \\\n",
       "0             0             0             0            0            0   \n",
       "1             1             0             0            0            0   \n",
       "2             0             0             0            0            0   \n",
       "3             0             0             0            0            0   \n",
       "4             0             0             0            1            0   \n",
       "\n",
       "   sachet_2g_8  sachet_2g_9  monthly_3g_6  monthly_3g_7  monthly_3g_8  \\\n",
       "0            0            0             1             1             1   \n",
       "1            2            0             0             0             0   \n",
       "2            0            1             0             0             0   \n",
       "3            0            0             0             0             0   \n",
       "4            0            0             0             0             0   \n",
       "\n",
       "   monthly_3g_9  sachet_3g_6  sachet_3g_7  sachet_3g_8  sachet_3g_9  \\\n",
       "0             0            0            0            0            0   \n",
       "1             0            0            0            0            0   \n",
       "2             0            0            0            0            0   \n",
       "3             0            0            0            0            0   \n",
       "4             0            0            0            0            0   \n",
       "\n",
       "   fb_user_6  fb_user_7  fb_user_8  fb_user_9   aon  aug_vbc_3g  jul_vbc_3g  \\\n",
       "0        1.0        1.0        1.0        NaN   968        30.4         0.0   \n",
       "1        NaN        1.0        1.0        NaN  1006         0.0         0.0   \n",
       "2        NaN        NaN        NaN        1.0  1103         0.0         0.0   \n",
       "3        NaN        NaN        NaN        NaN  2491         0.0         0.0   \n",
       "4        0.0        NaN        NaN        NaN  1526         0.0         0.0   \n",
       "\n",
       "   jun_vbc_3g  sep_vbc_3g  \n",
       "0      101.20        3.58  \n",
       "1        0.00        0.00  \n",
       "2        4.17        0.00  \n",
       "3        0.00        0.00  \n",
       "4        0.00        0.00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99999, 226)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99999 entries, 0 to 99998\n",
      "Data columns (total 226 columns):\n",
      " #   Column                    Dtype  \n",
      "---  ------                    -----  \n",
      " 0   mobile_number             int64  \n",
      " 1   circle_id                 int64  \n",
      " 2   loc_og_t2o_mou            float64\n",
      " 3   std_og_t2o_mou            float64\n",
      " 4   loc_ic_t2o_mou            float64\n",
      " 5   last_date_of_month_6      object \n",
      " 6   last_date_of_month_7      object \n",
      " 7   last_date_of_month_8      object \n",
      " 8   last_date_of_month_9      object \n",
      " 9   arpu_6                    float64\n",
      " 10  arpu_7                    float64\n",
      " 11  arpu_8                    float64\n",
      " 12  arpu_9                    float64\n",
      " 13  onnet_mou_6               float64\n",
      " 14  onnet_mou_7               float64\n",
      " 15  onnet_mou_8               float64\n",
      " 16  onnet_mou_9               float64\n",
      " 17  offnet_mou_6              float64\n",
      " 18  offnet_mou_7              float64\n",
      " 19  offnet_mou_8              float64\n",
      " 20  offnet_mou_9              float64\n",
      " 21  roam_ic_mou_6             float64\n",
      " 22  roam_ic_mou_7             float64\n",
      " 23  roam_ic_mou_8             float64\n",
      " 24  roam_ic_mou_9             float64\n",
      " 25  roam_og_mou_6             float64\n",
      " 26  roam_og_mou_7             float64\n",
      " 27  roam_og_mou_8             float64\n",
      " 28  roam_og_mou_9             float64\n",
      " 29  loc_og_t2t_mou_6          float64\n",
      " 30  loc_og_t2t_mou_7          float64\n",
      " 31  loc_og_t2t_mou_8          float64\n",
      " 32  loc_og_t2t_mou_9          float64\n",
      " 33  loc_og_t2m_mou_6          float64\n",
      " 34  loc_og_t2m_mou_7          float64\n",
      " 35  loc_og_t2m_mou_8          float64\n",
      " 36  loc_og_t2m_mou_9          float64\n",
      " 37  loc_og_t2f_mou_6          float64\n",
      " 38  loc_og_t2f_mou_7          float64\n",
      " 39  loc_og_t2f_mou_8          float64\n",
      " 40  loc_og_t2f_mou_9          float64\n",
      " 41  loc_og_t2c_mou_6          float64\n",
      " 42  loc_og_t2c_mou_7          float64\n",
      " 43  loc_og_t2c_mou_8          float64\n",
      " 44  loc_og_t2c_mou_9          float64\n",
      " 45  loc_og_mou_6              float64\n",
      " 46  loc_og_mou_7              float64\n",
      " 47  loc_og_mou_8              float64\n",
      " 48  loc_og_mou_9              float64\n",
      " 49  std_og_t2t_mou_6          float64\n",
      " 50  std_og_t2t_mou_7          float64\n",
      " 51  std_og_t2t_mou_8          float64\n",
      " 52  std_og_t2t_mou_9          float64\n",
      " 53  std_og_t2m_mou_6          float64\n",
      " 54  std_og_t2m_mou_7          float64\n",
      " 55  std_og_t2m_mou_8          float64\n",
      " 56  std_og_t2m_mou_9          float64\n",
      " 57  std_og_t2f_mou_6          float64\n",
      " 58  std_og_t2f_mou_7          float64\n",
      " 59  std_og_t2f_mou_8          float64\n",
      " 60  std_og_t2f_mou_9          float64\n",
      " 61  std_og_t2c_mou_6          float64\n",
      " 62  std_og_t2c_mou_7          float64\n",
      " 63  std_og_t2c_mou_8          float64\n",
      " 64  std_og_t2c_mou_9          float64\n",
      " 65  std_og_mou_6              float64\n",
      " 66  std_og_mou_7              float64\n",
      " 67  std_og_mou_8              float64\n",
      " 68  std_og_mou_9              float64\n",
      " 69  isd_og_mou_6              float64\n",
      " 70  isd_og_mou_7              float64\n",
      " 71  isd_og_mou_8              float64\n",
      " 72  isd_og_mou_9              float64\n",
      " 73  spl_og_mou_6              float64\n",
      " 74  spl_og_mou_7              float64\n",
      " 75  spl_og_mou_8              float64\n",
      " 76  spl_og_mou_9              float64\n",
      " 77  og_others_6               float64\n",
      " 78  og_others_7               float64\n",
      " 79  og_others_8               float64\n",
      " 80  og_others_9               float64\n",
      " 81  total_og_mou_6            float64\n",
      " 82  total_og_mou_7            float64\n",
      " 83  total_og_mou_8            float64\n",
      " 84  total_og_mou_9            float64\n",
      " 85  loc_ic_t2t_mou_6          float64\n",
      " 86  loc_ic_t2t_mou_7          float64\n",
      " 87  loc_ic_t2t_mou_8          float64\n",
      " 88  loc_ic_t2t_mou_9          float64\n",
      " 89  loc_ic_t2m_mou_6          float64\n",
      " 90  loc_ic_t2m_mou_7          float64\n",
      " 91  loc_ic_t2m_mou_8          float64\n",
      " 92  loc_ic_t2m_mou_9          float64\n",
      " 93  loc_ic_t2f_mou_6          float64\n",
      " 94  loc_ic_t2f_mou_7          float64\n",
      " 95  loc_ic_t2f_mou_8          float64\n",
      " 96  loc_ic_t2f_mou_9          float64\n",
      " 97  loc_ic_mou_6              float64\n",
      " 98  loc_ic_mou_7              float64\n",
      " 99  loc_ic_mou_8              float64\n",
      " 100 loc_ic_mou_9              float64\n",
      " 101 std_ic_t2t_mou_6          float64\n",
      " 102 std_ic_t2t_mou_7          float64\n",
      " 103 std_ic_t2t_mou_8          float64\n",
      " 104 std_ic_t2t_mou_9          float64\n",
      " 105 std_ic_t2m_mou_6          float64\n",
      " 106 std_ic_t2m_mou_7          float64\n",
      " 107 std_ic_t2m_mou_8          float64\n",
      " 108 std_ic_t2m_mou_9          float64\n",
      " 109 std_ic_t2f_mou_6          float64\n",
      " 110 std_ic_t2f_mou_7          float64\n",
      " 111 std_ic_t2f_mou_8          float64\n",
      " 112 std_ic_t2f_mou_9          float64\n",
      " 113 std_ic_t2o_mou_6          float64\n",
      " 114 std_ic_t2o_mou_7          float64\n",
      " 115 std_ic_t2o_mou_8          float64\n",
      " 116 std_ic_t2o_mou_9          float64\n",
      " 117 std_ic_mou_6              float64\n",
      " 118 std_ic_mou_7              float64\n",
      " 119 std_ic_mou_8              float64\n",
      " 120 std_ic_mou_9              float64\n",
      " 121 total_ic_mou_6            float64\n",
      " 122 total_ic_mou_7            float64\n",
      " 123 total_ic_mou_8            float64\n",
      " 124 total_ic_mou_9            float64\n",
      " 125 spl_ic_mou_6              float64\n",
      " 126 spl_ic_mou_7              float64\n",
      " 127 spl_ic_mou_8              float64\n",
      " 128 spl_ic_mou_9              float64\n",
      " 129 isd_ic_mou_6              float64\n",
      " 130 isd_ic_mou_7              float64\n",
      " 131 isd_ic_mou_8              float64\n",
      " 132 isd_ic_mou_9              float64\n",
      " 133 ic_others_6               float64\n",
      " 134 ic_others_7               float64\n",
      " 135 ic_others_8               float64\n",
      " 136 ic_others_9               float64\n",
      " 137 total_rech_num_6          int64  \n",
      " 138 total_rech_num_7          int64  \n",
      " 139 total_rech_num_8          int64  \n",
      " 140 total_rech_num_9          int64  \n",
      " 141 total_rech_amt_6          int64  \n",
      " 142 total_rech_amt_7          int64  \n",
      " 143 total_rech_amt_8          int64  \n",
      " 144 total_rech_amt_9          int64  \n",
      " 145 max_rech_amt_6            int64  \n",
      " 146 max_rech_amt_7            int64  \n",
      " 147 max_rech_amt_8            int64  \n",
      " 148 max_rech_amt_9            int64  \n",
      " 149 date_of_last_rech_6       object \n",
      " 150 date_of_last_rech_7       object \n",
      " 151 date_of_last_rech_8       object \n",
      " 152 date_of_last_rech_9       object \n",
      " 153 last_day_rch_amt_6        int64  \n",
      " 154 last_day_rch_amt_7        int64  \n",
      " 155 last_day_rch_amt_8        int64  \n",
      " 156 last_day_rch_amt_9        int64  \n",
      " 157 date_of_last_rech_data_6  object \n",
      " 158 date_of_last_rech_data_7  object \n",
      " 159 date_of_last_rech_data_8  object \n",
      " 160 date_of_last_rech_data_9  object \n",
      " 161 total_rech_data_6         float64\n",
      " 162 total_rech_data_7         float64\n",
      " 163 total_rech_data_8         float64\n",
      " 164 total_rech_data_9         float64\n",
      " 165 max_rech_data_6           float64\n",
      " 166 max_rech_data_7           float64\n",
      " 167 max_rech_data_8           float64\n",
      " 168 max_rech_data_9           float64\n",
      " 169 count_rech_2g_6           float64\n",
      " 170 count_rech_2g_7           float64\n",
      " 171 count_rech_2g_8           float64\n",
      " 172 count_rech_2g_9           float64\n",
      " 173 count_rech_3g_6           float64\n",
      " 174 count_rech_3g_7           float64\n",
      " 175 count_rech_3g_8           float64\n",
      " 176 count_rech_3g_9           float64\n",
      " 177 av_rech_amt_data_6        float64\n",
      " 178 av_rech_amt_data_7        float64\n",
      " 179 av_rech_amt_data_8        float64\n",
      " 180 av_rech_amt_data_9        float64\n",
      " 181 vol_2g_mb_6               float64\n",
      " 182 vol_2g_mb_7               float64\n",
      " 183 vol_2g_mb_8               float64\n",
      " 184 vol_2g_mb_9               float64\n",
      " 185 vol_3g_mb_6               float64\n",
      " 186 vol_3g_mb_7               float64\n",
      " 187 vol_3g_mb_8               float64\n",
      " 188 vol_3g_mb_9               float64\n",
      " 189 arpu_3g_6                 float64\n",
      " 190 arpu_3g_7                 float64\n",
      " 191 arpu_3g_8                 float64\n",
      " 192 arpu_3g_9                 float64\n",
      " 193 arpu_2g_6                 float64\n",
      " 194 arpu_2g_7                 float64\n",
      " 195 arpu_2g_8                 float64\n",
      " 196 arpu_2g_9                 float64\n",
      " 197 night_pck_user_6          float64\n",
      " 198 night_pck_user_7          float64\n",
      " 199 night_pck_user_8          float64\n",
      " 200 night_pck_user_9          float64\n",
      " 201 monthly_2g_6              int64  \n",
      " 202 monthly_2g_7              int64  \n",
      " 203 monthly_2g_8              int64  \n",
      " 204 monthly_2g_9              int64  \n",
      " 205 sachet_2g_6               int64  \n",
      " 206 sachet_2g_7               int64  \n",
      " 207 sachet_2g_8               int64  \n",
      " 208 sachet_2g_9               int64  \n",
      " 209 monthly_3g_6              int64  \n",
      " 210 monthly_3g_7              int64  \n",
      " 211 monthly_3g_8              int64  \n",
      " 212 monthly_3g_9              int64  \n",
      " 213 sachet_3g_6               int64  \n",
      " 214 sachet_3g_7               int64  \n",
      " 215 sachet_3g_8               int64  \n",
      " 216 sachet_3g_9               int64  \n",
      " 217 fb_user_6                 float64\n",
      " 218 fb_user_7                 float64\n",
      " 219 fb_user_8                 float64\n",
      " 220 fb_user_9                 float64\n",
      " 221 aon                       int64  \n",
      " 222 aug_vbc_3g                float64\n",
      " 223 jul_vbc_3g                float64\n",
      " 224 jun_vbc_3g                float64\n",
      " 225 sep_vbc_3g                float64\n",
      "dtypes: float64(179), int64(35), object(12)\n",
      "memory usage: 172.4+ MB\n"
     ]
    }
   ],
   "source": [
    "churn.info(verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed, there are 99999 rows and 226 columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Differentiating columns based upon type\n",
    "id_cols = ['mobile_number', 'circle_id']\n",
    "\n",
    "date_cols = ['last_date_of_month_6',\n",
    "             'last_date_of_month_7',\n",
    "             'last_date_of_month_8',\n",
    "             'last_date_of_month_9',\n",
    "             'date_of_last_rech_6',\n",
    "             'date_of_last_rech_7',\n",
    "             'date_of_last_rech_8',\n",
    "             'date_of_last_rech_9',\n",
    "             'date_of_last_rech_data_6',\n",
    "             'date_of_last_rech_data_7',\n",
    "             'date_of_last_rech_data_8',\n",
    "             'date_of_last_rech_data_9'\n",
    "            ]\n",
    "\n",
    "cat_cols =  ['night_pck_user_6',\n",
    "             'night_pck_user_7',\n",
    "             'night_pck_user_8',\n",
    "             'night_pck_user_9',\n",
    "             'fb_user_6',\n",
    "             'fb_user_7',\n",
    "             'fb_user_8',\n",
    "             'fb_user_9'\n",
    "            ]\n",
    "\n",
    "num_cols = [column for column in churn.columns if column not in id_cols + date_cols + cat_cols]\n",
    "\n",
    "print('Number of ID columns: {0}'.format(len(id_cols)))\n",
    "print('Number of Date columns: {0}'.format(len(date_cols)))\n",
    "print('Number of Category columns: {0}'.format(len(cat_cols)))\n",
    "print('Number of Numercical columns: {0}'.format(len(num_cols)))\n",
    "print('Total columns in dataset: {0}'.format(churn.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Percentage of missing values column-wise\n",
    "round(churn.isnull().sum()*100/churn.shape[0], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recharge related columns\n",
    "recharge_cols = ['total_rech_num_6', 'total_rech_num_7', 'total_rech_num_8', 'total_rech_num_9',\n",
    "                 'total_rech_amt_6', 'total_rech_amt_7', 'total_rech_amt_8', 'total_rech_amt_9',\n",
    "                 'max_rech_amt_6', 'max_rech_amt_7', 'max_rech_amt_8', 'max_rech_amt_9',\n",
    "                 'date_of_last_rech_6', 'date_of_last_rech_7', 'date_of_last_rech_8', 'date_of_last_rech_9',\n",
    "                 'last_day_rch_amt_6', 'last_day_rch_amt_7', 'last_day_rch_amt_8', 'last_day_rch_amt_9',\n",
    "                 ]\n",
    "\n",
    "churn[recharge_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn[recharge_cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the recharge amount for which recharge date is not present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn[recharge_cols].loc[churn[recharge_cols].date_of_last_rech_6.isnull(), \n",
    "                         ['total_rech_num_6', 'date_of_last_rech_6']].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn[recharge_cols].loc[churn[recharge_cols].date_of_last_rech_7.isnull(), \n",
    "                         ['total_rech_num_7', 'date_of_last_rech_7']].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn[recharge_cols].loc[churn[recharge_cols].date_of_last_rech_8.isnull(), \n",
    "                         ['total_rech_num_8', 'date_of_last_rech_8']].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn[recharge_cols].loc[churn[recharge_cols].date_of_last_rech_9.isnull(), \n",
    "                         ['total_rech_num_9', 'date_of_last_rech_9']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As observed, recharge value is 0 for when the recharge date is not present. So this is a meaningful missing, we leave it as it is since we will drop the recharge dates later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing data recharge features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We observed that the recharge data columns have around 74% null values, but we need this feature to later \n",
    "#filter out high value customers. Hence, replacing the null values with 0.0\n",
    "\n",
    "churn['total_rech_data_6'] = churn['total_rech_data_6'].replace(np.NaN,0.0)\n",
    "\n",
    "churn['total_rech_data_7'] = churn['total_rech_data_7'].replace(np.NaN,0.0)\n",
    "\n",
    "churn['total_rech_data_8'] = churn['total_rech_data_8'].replace(np.NaN,0.0)\n",
    "\n",
    "churn['av_rech_amt_data_6'] = churn['av_rech_amt_data_6'].replace(np.NaN,0.0)\n",
    "\n",
    "churn['av_rech_amt_data_7'] = churn['av_rech_amt_data_7'].replace(np.NaN,0.0)\n",
    "\n",
    "churn['av_rech_amt_data_8'] = churn['av_rech_amt_data_8'].replace(np.NaN,0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding new column total recharge amount for data: total_rech_amt_data for filtering High Value customer later\n",
    "\n",
    "churn['total_rech_amt_data_6'] = churn.av_rech_amt_data_6 * churn.total_rech_data_6\n",
    "\n",
    "churn['total_rech_amt_data_7'] = churn.av_rech_amt_data_7 * churn.total_rech_data_7\n",
    "\n",
    "churn['total_rech_amt_data_8'] = churn.av_rech_amt_data_8 * churn.total_rech_data_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can drop the avg recharge amount and total recharge amount columns\n",
    "cols_to_drop = ['total_rech_data_6', 'total_rech_data_7', 'total_rech_data_8',\n",
    "                'av_rech_amt_data_6', 'av_rech_amt_data_7', 'av_rech_amt_data_8']\n",
    "churn.drop(cols_to_drop, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing NaN in categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing Nan in categorical columns with new category '-1'\n",
    "churn[cat_cols] = churn[cat_cols].apply(lambda x: x.fillna('-1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(churn[cat_cols].isnull().sum() * 100 / churn.shape[0], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping columns with high percentage of missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping columns with more than 50% missing values\n",
    "MISSING_THRESHOLD = 0.5\n",
    "include_cols = list(churn.apply(lambda column: True if column.isnull().sum()/churn.shape[0] < MISSING_THRESHOLD else False))\n",
    "churn = churn.loc[:, include_cols]\n",
    "churn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_cols.count(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping ID and Date columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols = ['last_date_of_month_6',\n",
    "             'last_date_of_month_7',\n",
    "             'last_date_of_month_8',\n",
    "             'last_date_of_month_9',\n",
    "             'date_of_last_rech_6',\n",
    "             'date_of_last_rech_7',\n",
    "             'date_of_last_rech_8',\n",
    "             'date_of_last_rech_9'\n",
    "            ]\n",
    "churn = churn.drop(date_cols, axis=1)\n",
    "print(\"Shape after dropping: \", churn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing using IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_cols = ['max_rech_data_9', 'count_rech_2g_6', 'max_rech_data_6', 'av_rech_amt_data_7', 'count_rech_3g_7', \n",
    "                'arpu_3g_9', 'total_rech_data_8', 'arpu_2g_9', 'arpu_2g_8', 'count_rech_3g_8', 'count_rech_3g_9', \n",
    "                'total_rech_data_6', 'arpu_3g_8', 'max_rech_data_8', 'arpu_3g_6', 'count_rech_2g_8', 'arpu_2g_6', \n",
    "                'max_rech_data_7', 'total_rech_data_7', 'arpu_2g_7', 'total_rech_data_9', 'count_rech_2g_7', \n",
    "                'av_rech_amt_data_9', 'av_rech_amt_data_6', 'arpu_3g_7', 'count_rech_2g_9', 'av_rech_amt_data_8', \n",
    "                'count_rech_3g_6']\n",
    "num_cols = [x for x in num_cols if x not in dropped_cols]\n",
    "churn[num_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_cols = churn.columns\n",
    "imputer = IterativeImputer(max_iter = 2, random_state = 10)\n",
    "df_stg = churn.copy()\n",
    "df_stg.drop(id_cols, axis=1, inplace=True)\n",
    "churn_imputed = imputer.fit_transform(df_stg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert imputed numpy array to pandas dataframe\n",
    "churn_filtered = pd.DataFrame(churn_imputed, columns=churn_cols.drop(id_cols))\n",
    "print(churn_filtered.isnull().sum()*100/churn_filtered.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-value customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Indian and the southeast Asian market, approximately 80% of revenue comes from the top 20% customers (called high-value customers). Thus, if we can reduce churn of the high-value customers, we will be able to reduce significant revenue leakage.\n",
    "Those who have recharged with an amount more than or equal to X, where X is the 70th percentile of the average recharge amount in the first two months (the good phase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_filtered.info(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the total data recharge amount for June and July --> number of recharges * average recharge amount\n",
    "churn_filtered['avg_rech_amt_6_7'] = (churn_filtered.total_rech_amt_6 + churn_filtered.total_rech_amt_data_6\n",
    "                                    + churn_filtered.total_rech_amt_7 + churn_filtered.total_rech_amt_data_7) / 2\n",
    "churn_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the 70th percentile recharge amount\n",
    "print(\"Recharge amount at 70th percentile: {0}\".format(churn_filtered.avg_rech_amt_6_7.quantile(0.7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only those customers who have recharged their mobiles with more than or equal to 70th percentile amount\n",
    "churn_filtered = churn_filtered.loc[churn_filtered.avg_rech_amt_6_7 > churn_filtered.avg_rech_amt_6_7.quantile(0.7), :]\n",
    "churn_filtered = churn_filtered.reset_index(drop=True)\n",
    "churn_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_filtered = churn_filtered.drop('avg_rech_amt_6_7', axis = 1)\n",
    "churn_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As observed, after filteration, we are left with 29953 rows and 187 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving churn value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The customers who do not have any call recharge and data recharge are considered to have churned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate total incoming and outgoing minutes of usage\n",
    "churn_filtered['total_calls_mou_9'] = churn_filtered.total_ic_mou_9 + churn_filtered.total_og_mou_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate 2g and 3g data consumption\n",
    "churn_filtered['total_internet_mb_9'] =  churn_filtered.vol_2g_mb_9 + churn_filtered.vol_3g_mb_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create churn variable: those who have not used either calls or internet in the month of September are customers who have churned\n",
    "# 0 - not churn, 1 - churn\n",
    "churn_filtered['churn'] = churn_filtered.apply(lambda x: 1 if (x.total_calls_mou_9 == 0 and x.total_internet_mb_9 == 0) else 0, axis=1)\n",
    "churn_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete derived variables\n",
    "churn_filtered = churn_filtered.drop(['total_calls_mou_9', 'total_internet_mb_9'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print churn ratio\n",
    "print('Churn class')\n",
    "print(churn_filtered.churn.value_counts()*100/churn_filtered.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As observed, there is high class imbalance in churning. There is a 92:8 ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete columns that are related to 9th month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all features relating to 9th month\n",
    "churn_filtered = churn_filtered.filter(regex='[^9]$', axis=1)\n",
    "churn_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the categorical and numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all names that end with 9\n",
    "col_9_names = churn.filter(regex='9$', axis=1).columns\n",
    "print(col_9_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update num_cols and cat_cols column name list\n",
    "cat_cols = [col for col in cat_cols if col not in col_9_names]\n",
    "num_cols = [col for col in churn_filtered.columns if col not in cat_cols]\n",
    "\n",
    "print('Categorical columns: ' , len(cat_cols))\n",
    "print(cat_cols)\n",
    "print()\n",
    "print('Numerical columns: ' , len(num_cols))\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change columns types\n",
    "churn_filtered[num_cols] = churn_filtered[num_cols].apply(pd.to_numeric)\n",
    "churn_filtered[cat_cols] = churn_filtered[cat_cols].apply(lambda column: column.astype(\"category\"), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_filtered.info(verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New variables derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 8th month is the deciding month for the customer whether he will churn or not, let's try to find some differences between recharge and daat usage by the customer. We will calculate difference in behavior for all different types of column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_filtered['arpu_diff'] = churn_filtered.arpu_8 - ((churn_filtered.arpu_6 + churn_filtered.arpu_7)/2)\n",
    "\n",
    "churn_filtered['onnet_mou_diff'] = churn_filtered.onnet_mou_8 - ((churn_filtered.onnet_mou_6 + churn_filtered.onnet_mou_7)/2)\n",
    "\n",
    "churn_filtered['monthly_3g_diff'] = churn_filtered.monthly_3g_8 - ((churn_filtered.monthly_3g_6 + churn_filtered.monthly_3g_7)/2)\n",
    "\n",
    "churn_filtered['roam_ic_mou_diff'] = churn_filtered.roam_ic_mou_8 - ((churn_filtered.roam_ic_mou_6 + churn_filtered.roam_ic_mou_7)/2)\n",
    "\n",
    "churn_filtered['roam_og_mou_diff'] = churn_filtered.roam_og_mou_8 - ((churn_filtered.roam_og_mou_6 + churn_filtered.roam_og_mou_7)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove columns with no variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in churn_filtered.columns:\n",
    "    if churn_filtered[col].nunique() == 1:\n",
    "        print(\"Column\", col ,\"has no variance and contains only \", churn_filtered[col].nunique(),\" unique value\")\n",
    "        print(\"Dropping the column\", col)\n",
    "        print()\n",
    "        churn_filtered.drop(col, axis = 1, inplace = True)\n",
    "\n",
    "print(\"Shape of the updated dataset:\", churn_filtered.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the churn ratio\n",
    "plt.figure(figsize = [5,5])\n",
    "plt.title('Churn', fontsize = 15)\n",
    "churn_filtered.churn.value_counts(normalize=True).plot.pie(autopct='%1.1f%%', labels=['Churned', 'Not churned'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As observed, there is a class imbalance in the target variable, i.e., churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(churn_filtered.arpu_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We observe a normal plot for average revenue per user difference (8th - avg(6th and 7th)) with mean zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [15,8])\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Monthly 3g difference - Churned')\n",
    "sns.distplot(churn_filtered.loc[churn_filtered.churn == 1].monthly_3g_diff, kde = True)\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Monthly 3g difference - Not churned')\n",
    "sns.distplot(churn_filtered.loc[churn_filtered.churn == 0].monthly_3g_diff, kde = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For churned users we observe that values <=0 have higher frequency density, where as in non-churn the frequency density is higher on the positive side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenure_data = churn_filtered.copy()\n",
    "plt.figure(figsize=(14,8))\n",
    "# aon --> Age on network - number of days the customer is using the operator's network\n",
    "tenure_data['tenure'] = tenure_data['aon'] / 30\n",
    "sns.distplot(tenure_data['tenure'], hist = True, kde = False,\n",
    "             bins = int(180/5), color = 'blue', \n",
    "             hist_kws = {'edgecolor':'red'},\n",
    "             kde_kws = {'linewidth': 4})\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.xlabel('Tenure in Months')\n",
    "plt.title('Customers Vs Tenure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As observed, customers tend to leave the operator service after sometime. The customers are not loyal to the service, maybe because of the schemes or service or network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to observer behavior difference in 6th, 7th and 8th month\n",
    "def plot_box_chart(attribute):\n",
    "    plt.figure(figsize=(20,16))\n",
    "    df = churn_filtered\n",
    "    plt.subplot(2,3,1)\n",
    "    plt.title('Month 6', fontsize = 15)\n",
    "    sns.boxplot(data = df, y = attribute + \"_6\", x = \"churn\", hue = \"churn\",\n",
    "                showfliers = False, palette= (\"plasma\"))\n",
    "    plt.subplot(2,3,2)\n",
    "    plt.title('Month 7', fontsize = 15)\n",
    "    sns.boxplot(data = df, y = attribute + \"_7\", x = \"churn\", hue = \"churn\",\n",
    "                showfliers = False, palette = (\"plasma\"))\n",
    "    plt.subplot(2,3,3)\n",
    "    plt.title('Month 8', fontsize = 15)\n",
    "    sns.boxplot(data = df, y = attribute + \"_8\", x = \"churn\", hue = \"churn\",\n",
    "                showfliers = False, palette = (\"plasma\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recharge amount vs churn\n",
    "plot_box_chart('max_rech_amt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As observed, the value for max_rech_amt has dropped drastically in the 8th month for customers who have churned. For customers who have not churned, the max_rech_amt remains somewhat constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box_chart('total_rech_amt_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As observed, there is a huge drop in total recharge amount for data in the 8th month (action phase) for churned customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_filtered.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x = churn_filtered.roam_ic_mou_diff, y = churn_filtered.roam_og_mou_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x = 'churn', y = 'arpu_diff', data = churn_filtered, showfliers = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [8, 8])\n",
    "plt.title('Number recharge vs Data recharge in month 8')\n",
    "sns.scatterplot(x = 'max_rech_amt_8', y = 'total_rech_amt_data_8', hue = 'churn', data = churn_filtered)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As observed, there are some high-value customers who had high max_rech_amt in months na dstill churned. One possible reason for it can be a better scheme from competetive services. Also, all the churned customers have low total_rech_amt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [8, 8])\n",
    "plt.title('Arpu difference vs onnet_mou difference (8th - AVG(6th + 7th))')\n",
    "sns.scatterplot(x = 'arpu_diff', y = 'onnet_mou_diff', hue = 'churn', data = churn_filtered)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Correlation matrix\n",
    "# figure size\n",
    "plt.figure(figsize=(15,15))\n",
    "corr_matrix = churn_filtered.corr().abs()\n",
    "# heatmap\n",
    "sns.heatmap(corr_matrix, cmap=\"YlGnBu\", annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove highly correlated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting columns which have absolute correlation greater than 60%\n",
    "# Create correlation matrix\n",
    "corr_matrix = churn_filtered.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find features with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.6)]\n",
    "\n",
    "# Drop features \n",
    "churn_filtered.drop(to_drop, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the category and numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = churn_filtered.select_dtypes('category').columns\n",
    "num_cols = [col for col in churn_filtered.columns if col not in cat_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dropping highly skewed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewnessThreshold = 80\n",
    "def RemoveSkewedColumn(col, type):\n",
    "    if col == 'churn':\n",
    "        return\n",
    "    count_dict = (round(churn_filtered[col].value_counts(normalize = True) * 100, 2)).to_dict()\n",
    "    IsSkewed = list(map(lambda x: x >= 80, count_dict.values()))\n",
    "    if IsSkewed.count(True) > 0:\n",
    "        print(col, \"Skewness:\")\n",
    "        #print(count_dict)\n",
    "        churn_filtered.drop(col, axis = 1, inplace = True)\n",
    "        print('Due to high skewness, this column was dropped')\n",
    "        print()\n",
    "    else:\n",
    "        if type == 'category':\n",
    "            #change type of column to category\n",
    "            churn_filtered[col] = churn_filtered[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    RemoveSkewedColumn(col, 'numeric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = churn_filtered.select_dtypes('category').columns\n",
    "num_cols = [col for col in churn_filtered.columns if col not in cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### redraw the correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Correlation matrix\n",
    "# figure size\n",
    "plt.figure(figsize=(15,15))\n",
    "corr_matrix = churn_filtered.corr().abs()\n",
    "# heatmap\n",
    "sns.heatmap(corr_matrix, cmap=\"YlGnBu\", annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treating outliers using IQR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using IQR methodology to cap outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    Q1 = churn_filtered[col].quantile(0.25)\n",
    "    Q3 = churn_filtered[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    if(len(churn_filtered[num_cols[0]][churn_filtered[num_cols[0]] < (Q1 - 1.5 * IQR)]) > 0 \n",
    "       or len(churn_filtered[num_cols[0]][churn_filtered[num_cols[0]] > (Q3 + 1.5 * IQR)]) > 0):\n",
    "        print('Found outliers in column: ' + col)\n",
    "        churn_filtered[num_cols[0]][churn_filtered[num_cols[0]] < (Q1 - 1.5 * IQR)] = Q1 - 1.5 * IQR\n",
    "        churn_filtered[num_cols[0]][churn_filtered[num_cols[0]] > (Q3 + 1.5 * IQR)] = Q3 + 1.5 * IQR\n",
    "        print('Capped outliers in column \"' + col + '\" using IQR method' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_column_names = [\"Model\", \"Train Accuracy\", \"Test Accuracy\", \"Train Recall\", \"Test Recall\", \"Test F1 Score\"]\n",
    "\n",
    "model_results = pd.DataFrame(columns = result_column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummification of categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dummy variable for some of the categorical variables and dropping the first one.\n",
    "dummyVariables = pd.get_dummies(churn_filtered[cat_cols], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the results to the master dataframe\n",
    "churn_filtered = pd.concat([churn_filtered, dummyVariables], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_filtered.drop(cat_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data set in 7:3 ratio for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataframe to 70% train and 30% test\n",
    "df_train, df_test = train_test_split(churn_filtered, train_size=0.7, random_state=100)\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['churn']\n",
    "X_train = df_train.drop('churn', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = df_test['churn']\n",
    "X_test = df_test.drop('churn', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a minmax scaler to scale data in train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling \n",
    "scaler = MinMaxScaler()\n",
    "num_cols.remove('churn')\n",
    "X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model using RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sklearn \n",
    "logreg = LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE(logreg, 15)             # running RFE with 15 variables as output\n",
    "rfe = rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## list columns that rfe predicted as useful\n",
    "list(zip(X_train.columns, rfe.support_, rfe.ranking_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get supported columns from RFE\n",
    "col = X_train.columns[rfe.support_]\n",
    "col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing the model with StatsModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sm = sm.add_constant(X_train[col])\n",
    "logm2 = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\n",
    "res = logm2.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train[col].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### drop vol_3g_mb_diff since this has p value greater than expected\n",
    "col = col.drop('loc_og_t2f_mou_6')\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sm = sm.add_constant(X_train[col])\n",
    "logm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\n",
    "res = logm2.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check VIf values again\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train[col].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### drop total_rech_amt_data_7 since this has p value greater than expected\n",
    "col = col.drop('monthly_3g_diff', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sm = sm.add_constant(X_train[col])\n",
    "logm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\n",
    "res = logm2.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check VIf values again\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train[col].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop loc_og_t2m_mou_diff as it has high p value\n",
    "col = col.drop('loc_ic_t2f_mou_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sm = sm.add_constant(X_train[col])\n",
    "logm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\n",
    "res = logm2.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check VIf values again\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train[col].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop loc_og_t2m_mou_diff as it has high p value\n",
    "col = col.drop('roam_og_mou_diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sm = sm.add_constant(X_train[col])\n",
    "logm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\n",
    "res = logm2.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check VIf values again\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train[col].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop loc_og_t2m_mou_diff as it has high p value\n",
    "col = col.drop('spl_og_mou_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sm = sm.add_constant(X_train[col])\n",
    "logm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\n",
    "res = logm2.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check VIf values again\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train[col].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop loc_og_t2m_mou_diff as it has high p value\n",
    "col = col.drop('arpu_diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sm = sm.add_constant(X_train[col])\n",
    "logm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\n",
    "res = logm2.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check VIf values again\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train[col].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop loc_og_t2m_mou_diff as it has high p value\n",
    "col = col.drop('onnet_mou_diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sm = sm.add_constant(X_train[col])\n",
    "logm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\n",
    "res = logm2.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check VIf values again\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train[col].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop loc_og_t2m_mou_diff as it has high p value\n",
    "col = col.drop('loc_ic_t2t_mou_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sm = sm.add_constant(X_train[col])\n",
    "logm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\n",
    "res = logm2.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check VIf values again\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train[col].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalCols = X_train[col].columns\n",
    "X_test_log = X_test[finalCols]\n",
    "X_test_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sklearn\n",
    "logregfinal = LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logregfinal.fit(X_train[finalCols], y_train)\n",
    "y_test_pred = logregfinal.predict(X_test_log)\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of model on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_lr_pred = logregfinal.predict(X_train[finalCols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create onfusion matrix\n",
    "y_train_lr_pred_final = pd.DataFrame({'Converted': y_train.values, 'Converted_Prob': y_train_lr_pred})\n",
    "y_train_lr_pred_final['LeadNumber'] = y_train.index\n",
    "\n",
    "y_train_lr_pred_final['predicted'] = y_train_lr_pred_final.Converted_Prob.map(lambda x: 1 if x > 0.35 else 0)\n",
    "y_train_lr_pred_final.head()\n",
    "\n",
    "# Let's check the overall accuracy.\n",
    "rfe_log_train_acc = round(metrics.accuracy_score(y_train_lr_pred_final.Converted, y_train_lr_pred_final.predicted), 2)\n",
    "rfe_log_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the sensitivity/recall of our logistic regression model\n",
    "rfe_log_train_recall = recall_score(y_train_lr_pred_final.Converted, y_train_lr_pred_final.predicted)\n",
    "rfe_log_train_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create onfusion matrix\n",
    "y_pred_final = pd.DataFrame({'Converted':y_test.values, 'Converted_Prob':y_test_pred})\n",
    "y_pred_final['LeadNumber'] = y_test.index\n",
    "\n",
    "\n",
    "y_pred_final['predicted'] = y_pred_final.Converted_Prob.map(lambda x: 1 if x > 0.35 else 0)\n",
    "y_pred_final.head()\n",
    "\n",
    "# Let's check the overall accuracy.\n",
    "rfe_log_test_acc = round(metrics.accuracy_score(y_pred_final.Converted, y_pred_final.predicted), 2)\n",
    "rfe_log_test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We observe 71% accuracy on train data and 72% accuracy on test data at 0.35 probability cut off. Hence, we move forward for calculating the sensitivity of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confusion_log = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.predicted )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion_log[1,1] # true positive \n",
    "TN = confusion_log[0,0] # true negatives\n",
    "FP = confusion_log[0,1] # false positives\n",
    "FN = confusion_log[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the sensitivity/recall of our logistic regression model\n",
    "rfe_log_test_recall = recall_score(y_pred_final.Converted, y_pred_final.predicted)\n",
    "rfe_log_test_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us calculate specificity\n",
    "TN / float(TN+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision\n",
    "precision_score(y_pred_final.Converted, y_pred_final.predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 score\n",
    "rfe_test_f1 = f1_score(y_pred_final.Converted, y_pred_final.predicted)\n",
    "rfe_test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_column_names = [\"Model\", \"Train Accuracy\", \"Test Accuracy\", \"Train Recall\", \"Test Recall\", \"Test F1 Score\"]\n",
    "rfe_temp_results = {\n",
    "        result_column_names[0]:'Logistic Regression with RFE', \n",
    "        result_column_names[1]:rfe_log_train_acc,\n",
    "        result_column_names[2]:rfe_log_test_acc,\n",
    "        result_column_names[3]:rfe_log_train_recall,\n",
    "        result_column_names[4]:rfe_log_test_recall,\n",
    "        result_column_names[5]:rfe_test_f1,\n",
    "}\n",
    "model_results = model_results.append(rfe_temp_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(random_state = 42)\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.round(pca.explained_variance_ratio_.cumsum(), 4)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature variance\n",
    "features = range(pca.n_components_)\n",
    "cumulative_variance = np.round(np.cumsum(pca.explained_variance_ratio_)*100, decimals=4)\n",
    "plt.figure(figsize=(175/20,100/20)) # 100 elements on y-axis; 175 elements on x-axis; 20 is normalising factor\n",
    "plt.plot(cumulative_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the explained variance ratio for each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making a scree plot for the explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_cumu = np.cumsum(pca.explained_variance_ratio_)\n",
    "var_cumu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[12,8])\n",
    "plt.vlines(x=6, ymax=2, ymin=0, colors=\"r\", linestyles=\"--\")\n",
    "plt.hlines(y=0.95, xmax=30, xmin=0, colors=\"g\", linestyles=\"--\")\n",
    "plt.plot(var_cumu)\n",
    "plt.ylabel(\"Cumulative variance explained\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_final = PCA(0.99, svd_solver='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_pca = pca_final.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = np.corrcoef(df_train_pca.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the heatmap of the corr matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,15])\n",
    "sns.heatmap(corrmat, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the transformation on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_pca = pca_final.transform(X_test)\n",
    "df_test_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the explained variance ratio for each component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning - PCA and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression - the class weight is used to handle class imbalance - it adjusts the cost function\n",
    "logistic = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "# hyperparameter space\n",
    "params = {'C': [0.08, 0.09, 0.1, 0.5, 1, 2, 3, 4, 5, 10], 'penalty': ['l1', 'l2']}\n",
    "\n",
    "# create 5 folds\n",
    "folds = KFold(n_splits = 5, shuffle = True, random_state = 4)\n",
    "\n",
    "# create gridsearch object\n",
    "model = GridSearchCV(estimator=logistic, cv=folds, param_grid=params, scoring='recall', n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "model.fit(df_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation results\n",
    "pd.DataFrame(model.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best hyperparameters\n",
    "print(\"Best AUC: \", model.best_score_)\n",
    "print(\"Best hyperparameters: \", model.best_params_)\n",
    "print(model.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict churn on train data\n",
    "y_pred_train = model.predict(df_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check area under curve\n",
    "y_pred_prob_train = model.predict_proba(df_train_pca)[:, 1]\n",
    "print(\"AUC:    \\t\", round(roc_auc_score(y_train, y_pred_prob_train),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create onfusion matrix\n",
    "confusion = confusion_matrix(y_train, y_pred_train)\n",
    "\n",
    "TP = confusion[1,1] # true positive \n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the sensitivity/recall of our logistic regression model on train data\n",
    "log_pca_train_recall = recall_score(y_train,y_pred_train )\n",
    "log_pca_train_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy on train data\n",
    "log_pca_train_acc = round(metrics.accuracy_score(y_train, y_pred_train), 2)\n",
    "log_pca_train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict churn on test data\n",
    "y_pred = model.predict(df_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check area under curve\n",
    "y_pred_prob = model.predict_proba(df_test_pca)[:, 1]\n",
    "print(\"AUC:    \\t\", round(roc_auc_score(y_test, y_pred_prob),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We observe 78% AUC on train data and 80% AUC on test data. Hence, this model is acceptable. We can move forward with calculating the sensitivity of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create onfusion matrix\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion[1,1] # true positive \n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy on test data\n",
    "log_pca_test_acc = round(metrics.accuracy_score(y_test, y_pred), 2)\n",
    "log_pca_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sensitivity/recall on test data\n",
    "log_pca_test_recall = recall_score(y_test,y_pred )\n",
    "log_pca_test_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us calculate specificity\n",
    "specificity = TN / float(TN+FP)\n",
    "print(\"Specificity: \\t\", round(specificity, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 score\n",
    "log_pca_test_f1 = f1_score(y_test, y_pred)\n",
    "log_pca_test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the results.\n",
    "log_pca_results = {\n",
    "        result_column_names[0]: 'Logistic Regression with PCA', \n",
    "        result_column_names[1]: log_pca_train_acc,\n",
    "        result_column_names[2]: log_pca_test_acc,\n",
    "        result_column_names[3]: log_pca_train_recall,\n",
    "        result_column_names[4]: log_pca_test_recall,\n",
    "        result_column_names[5]: log_pca_test_f1,\n",
    "}\n",
    "model_results = model_results.append(log_pca_results, ignore_index=True)\n",
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Logistic regression + PCA, we have attained a sensitivity of 81%. Let's now build a decision tree with PCA to see if we can do better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier with PCA and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsTree = DecisionTreeClassifier(class_weight = 'balanced')\n",
    "dsTreeParams = {\n",
    "            'max_depth': [10, 20, 25, 28, 30, 35, 40],\n",
    "            'min_samples_leaf': [50, 100, 200, 250, 300]}\n",
    "\n",
    "dsFolds = KFold(n_splits = 5, random_state = 42)\n",
    "\n",
    "dsTreeModel = GridSearchCV(estimator = dsTree, cv = dsFolds, param_grid = dsTreeParams, scoring='recall', \n",
    "                           verbose = 1, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "dsTreeModel.fit(df_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best hyperparameters\n",
    "print(\"Best AUC: \", dsTreeModel.best_score_)\n",
    "print(\"Best hyperparameters: \", dsTreeModel.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsTreeTrainPred = dsTreeModel.predict(df_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create onfusion matrix\n",
    "dsTreeTrainconfusion = confusion_matrix(y_train, dsTreeTrainPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP_ds = dsTreeTrainconfusion[1,1] # true positive \n",
    "TN_ds = dsTreeTrainconfusion[0,0] # true negatives\n",
    "FP_ds = dsTreeTrainconfusion[0,1] # false positives\n",
    "FN_ds = dsTreeTrainconfusion[1,0] # false negatives\n",
    "\n",
    "# Let's see the sensitivity of our decision tree model\n",
    "sensitivity_ds = TP_ds / float(TP_ds + FN_ds)\n",
    "print(\"Sensitivity: \\t\", round(sensitivity_ds, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sensitivity/recall on train data\n",
    "dsTree_train_recall = recall_score(y_train,dsTreeTrainPred )\n",
    "dsTree_train_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy on train data\n",
    "dsTree_train_acc = round(metrics.accuracy_score(y_train,dsTreeTrainPred), 2)\n",
    "dsTree_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check area under curve\n",
    "y_train_pred_prob_ds = dsTreeModel.predict_proba(df_train_pca)[:, 1]\n",
    "print(\"AUC:    \\t\", round(roc_auc_score(y_train, y_train_pred_prob_ds),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict churn on test data\n",
    "dsTreePred = dsTreeModel.predict(df_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create onfusion matrix\n",
    "dsTreeconfusion = confusion_matrix(y_test, dsTreePred)\n",
    "dsTreeconfusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP_ds = dsTreeconfusion[1,1] # true positive \n",
    "TN_ds = dsTreeconfusion[0,0] # true negatives\n",
    "FP_ds = dsTreeconfusion[0,1] # false positives\n",
    "FN_ds = dsTreeconfusion[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the sensitivity of our decision tree model\n",
    "sensitivity_ds = TP_ds / float(TP_ds+FN_ds)\n",
    "print(\"Sensitivity: \\t\", round(sensitivity_ds, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us calculate specificity\n",
    "specificity_ds = TN_ds / float(TN_ds+FP_ds)\n",
    "print(\"Specificity: \\t\", round(specificity_ds, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sensitivity/recall on train data\n",
    "dsTree_test_recall = recall_score(y_test, dsTreePred)\n",
    "dsTree_test_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy on train data\n",
    "dsTree_test_acc = round(metrics.accuracy_score(y_test, dsTreePred), 2)\n",
    "dsTree_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check area under curve\n",
    "y_pred_prob_ds = dsTreeModel.predict_proba(df_test_pca)[:, 1]\n",
    "print(\"AUC:    \\t\", round(roc_auc_score(y_test, y_pred_prob_ds),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 score\n",
    "dsTree_test_f1 = f1_score(y_test, dsTreePred)\n",
    "dsTree_test_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We observe 78% sensitivity on train data and 74% sensitivity on test data. Hence, we can accept this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the results.\n",
    "\n",
    "# result_column_names = [\"Model\", \"Train Accuracy\", \"Test Accuracy\", \"Train Recall\", \"Test Recall\", \"Test F1 Score\"]\n",
    "dsTree_results = {\n",
    "        result_column_names[0]: 'Logistic Regression with PCA', \n",
    "        result_column_names[1]: dsTree_train_acc,\n",
    "        result_column_names[2]: dsTree_test_acc,\n",
    "        result_column_names[3]: dsTree_train_recall,\n",
    "        result_column_names[4]: dsTree_test_recall,\n",
    "        result_column_names[5]: dsTree_test_f1,\n",
    "}\n",
    "model_results = model_results.append(dsTree_results, ignore_index=True)\n",
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As observed, decision tree + PCA performs equally well on test data with 80% sesitivity. Let's build a random forest and try to improve the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest - the class weight is used to handle class imbalance - it adjusts the cost function\n",
    "forest = RandomForestClassifier(class_weight='balanced', n_jobs = -1)\n",
    "\n",
    "# hyperparameter space\n",
    "params = { \n",
    "            'max_depth': [25, 28, 30, 35],\n",
    "            'min_samples_leaf': [200, 300, 400],\n",
    "            'n_estimators': [100, 150, 200]}\n",
    "\n",
    "# create 5 folds\n",
    "folds = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "\n",
    "# create gridsearch object\n",
    "rf_model = GridSearchCV(estimator=forest, cv=folds, param_grid=params, scoring='recall', n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best hyperparameters\n",
    "print(\"Best AUC: \", rf_model.best_score_)\n",
    "print(\"Best hyperparameters: \", rf_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the random forest from best parameters obtained above\n",
    "rf_best_model = RandomForestClassifier(\n",
    "    n_estimators = 200, \n",
    "    max_depth = 30,\n",
    "    min_samples_leaf = 400,\n",
    "    class_weight = 'balanced', \n",
    "    oob_score = True, \n",
    "    random_state = 4, \n",
    "    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "rf_best_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict churn on train data\n",
    "y_rf_train_pred = rf_best_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create onfusion matrix\n",
    "confusion = confusion_matrix(y_train, y_rf_train_pred)\n",
    "\n",
    "TP = confusion[1,1] # true positive \n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives\n",
    "\n",
    "# Let's see the sensitivity of our random forest model\n",
    "sensitivity = TP / float(TP+FN)\n",
    "print(\"Sensitivity: \\t\", round(sensitivity, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict churn on test data\n",
    "y_rf_pred = rf_best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create onfusion matrix\n",
    "confusion = confusion_matrix(y_test, y_rf_pred)\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion[1,1] # true positive \n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the sensitivity of our random forest model\n",
    "sensitivity = TP / float(TP+FN)\n",
    "print(\"Sensitivity: \\t\", round(sensitivity, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us calculate specificity\n",
    "specificity = TN / float(TN+FP)\n",
    "print(\"Specificity: \\t\", round(specificity, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check area under curve\n",
    "y_rf_pred_prob = rf_best_model.predict_proba(X_test)[:, 1]\n",
    "print(\"AUC:    \\t\", round(roc_auc_score(y_test, y_rf_pred_prob),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We observe 82% sensitivity on train data and 84% sensitivity on test data. Hence, we can accept this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note - We will be using this model later to derive the feature importance due to high interpretibility of Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest with PCA and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest - the class weight is used to handle class imbalance - it adjusts the cost function\n",
    "forest_pca = RandomForestClassifier(class_weight='balanced', n_jobs = -1)\n",
    "\n",
    "# hyperparameter space\n",
    "params_pca = {\n",
    "            'max_depth': [25, 28, 30, 35, 40],\n",
    "            'min_samples_leaf': [100, 200, 300, 400],\n",
    "            'n_estimators': [50, 100, 150, 200]\n",
    "}\n",
    "\n",
    "# create 5 folds\n",
    "folds_pca = KFold(n_splits = 5, shuffle = True, random_state = 4)\n",
    "\n",
    "# create gridsearch object\n",
    "rf_model_pca = GridSearchCV(estimator=forest, cv=folds, param_grid=params, scoring='recall', n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "rf_model_pca.fit(df_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_pca.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf_train_pca = rf_model_pca.predict(df_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create onfusion matrix\n",
    "confusion_rf_pca = confusion_matrix(y_train, y_pred_rf_train_pca)\n",
    "\n",
    "TP = confusion_rf_pca[1,1] # true positive \n",
    "TN = confusion_rf_pca[0,0] # true negatives\n",
    "FP = confusion_rf_pca[0,1] # false positives\n",
    "FN = confusion_rf_pca[1,0] # false negatives\n",
    "\n",
    "# Let's see the sensitivity of our logistic regression model\n",
    "sensitivity_rf_pca = TP / float(TP+FN)\n",
    "print(\"Sensitivity: \\t\", round(sensitivity_rf_pca, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf_pca = rf_model_pca.predict(df_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create onfusion matrix\n",
    "confusion_rf_pca = confusion_matrix(y_test, y_pred_rf_pca)\n",
    "confusion_rf_pca "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion_rf_pca[1,1] # true positive \n",
    "TN = confusion_rf_pca[0,0] # true negatives\n",
    "FP = confusion_rf_pca[0,1] # false positives\n",
    "FN = confusion_rf_pca[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the sensitivity of our logistic regression model\n",
    "sensitivity_rf_pca = TP / float(TP+FN)\n",
    "print(\"Sensitivity: \\t\", round(sensitivity_rf_pca, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us calculate specificity\n",
    "specificity_rf_pca = TN / float(TN+FP)\n",
    "print(\"Specificity: \\t\", round(specificity_rf_pca, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check area under curve\n",
    "y_pred_prob_rf_pca = rf_model_pca.predict_proba(df_test_pca)[:, 1]\n",
    "print(\"AUC:    \\t\", round(roc_auc_score(y_test, y_pred_prob_rf_pca),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We observe 77% sensitivity on train data and 79% sensitivity on test data. Hence, we can accept this model. But our previous model have performed slightly better than this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a KFold object \n",
    "folds_xg = 5\n",
    "\n",
    "# specify range of hyperparameters\n",
    "param_grid_xg = {\n",
    "            'learning_rate': [0.1, 0.2, 0.3], \n",
    "             'subsample': [0.3, 0.4, 0.5],\n",
    "             'max_depth': [2, 5],\n",
    "             'scale_pos_weight': [10, 20, 30, 40]\n",
    "            }          \n",
    "\n",
    "\n",
    "# specify model\n",
    "xgb_model = XGBClassifier(n_estimators=200, scale_pos_weight = 11.5)\n",
    "\n",
    "# set up GridSearchCV()\n",
    "model_cv_xg = GridSearchCV(estimator = xgb_model, \n",
    "                        param_grid = param_grid_xg, \n",
    "                        scoring= 'recall',\n",
    "                        cv = folds_xg, \n",
    "                        n_jobs = -1,\n",
    "                        verbose = 1,\n",
    "                        return_train_score=True)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model_cv_xg.fit(df_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We can get accuracy of **'+str(round(model_cv_xg.best_score_,2))+'** using '+str(model_cv_xg.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cv_xg.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xg_train = model_cv_xg.predict(df_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create onfusion matrix\n",
    "confusion_xg = confusion_matrix(y_train, y_pred_xg_train)\n",
    "\n",
    "TP = confusion_xg[1,1] # true positive \n",
    "TN = confusion_xg[0,0] # true negatives\n",
    "FP = confusion_xg[0,1] # false positives\n",
    "FN = confusion_xg[1,0] # false negatives\n",
    "\n",
    "# Let's see the sensitivity of our XGBoost model\n",
    "sensitivity_xg = TP / float(TP+FN)\n",
    "print(\"Sensitivity: \\t\", round(sensitivity_xg, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xg = model_cv_xg.predict(df_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create onfusion matrix\n",
    "confusion_xg = confusion_matrix(y_test, y_pred_xg)\n",
    "confusion_xg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion_xg[1,1] # true positive \n",
    "TN = confusion_xg[0,0] # true negatives\n",
    "FP = confusion_xg[0,1] # false positives\n",
    "FN = confusion_xg[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the sensitivity of our XGBoost model\n",
    "sensitivity_xg = TP / float(TP+FN)\n",
    "print(\"Sensitivity: \\t\", round(sensitivity_xg, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us calculate specificity\n",
    "specificity_xg = TN / float(TN+FP)\n",
    "print(\"Specificity: \\t\", round(specificity_xg, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We observe 96% sensitivity on train data and 93% sensitivity on test data. Hence, we can accept this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can derive the feature importance from our Random Forest model which was built without PCA since the model already has feauture importance inbuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictors\n",
    "features = churn_filtered.drop('churn', axis=1).columns\n",
    "\n",
    "# feature_importance\n",
    "importance = rf_best_model.feature_importances_\n",
    "\n",
    "# create dataframe\n",
    "feature_importance = pd.DataFrame({'variables': features, 'importance_percentage': importance*100})\n",
    "feature_importance = feature_importance[['variables', 'importance_percentage']]\n",
    "\n",
    "# sort features\n",
    "feature_importance = feature_importance.sort_values('importance_percentage', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretations & Conslusions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Average revenue per user seems to be most important feature in determining churn prediction\n",
    "- Incoming and Outgoing Calls on romaing for 8th month are strong indicators of churn behaviour\n",
    "- Roaming packs for both incoming and outgoing are strong predictors for churning\n",
    "- Finally the data recharge for the 8th month is also an important feature for churn prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In course of this assignment, we created several models:\n",
    "- Logistic regression\n",
    "- Logistic regression with PCA\n",
    "- Decision tree with PCA\n",
    "- Random forest\n",
    "- Random forest with PCA\n",
    "- XGBoost with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models result summary \n",
    "model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, the business aims to predict the customers who will churn, hence, our model should have highest sensitivity as possible.\n",
    "The best model observed is XGBoost with 93% sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
